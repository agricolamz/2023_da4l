[["index.html", "Анализ данных для лингвистов 1 О курсе 1.1 Используемые пакеты 1.2 Домашние задания", " Анализ данных для лингвистов Г. А. Мороз 1 О курсе Материалы для курса Анализа данных для лингвистов, Школа лингвистики НИУ ВШЭ. вместо первой лекции можно посмотреть запись лекции 2021.01.13 вместо второй лекции можно посмотреть запись лекции 2021.01.15 вместо третей лекции можно посмотреть запись лекции 2021.01.20 вместо четвертой лекции можно посмотреть запись лекции 2021.01.22 вместо пятой лекции можно посмотреть запись лекции 2021.01.27 вместо шестой лекции можно посмотреть запись лекции 2021.01.29 вместо седьмой лекции можно посмотреть запись лекции 2021.02.03 вместо восьмой лекции можно посмотреть запись лекции 2021.02.05 вместо девятой лекции можно посмотреть запись лекции 2021.02.10 вместо десятой лекции можно посмотреть запись лекции 2021.02.12 вместо одиннадцатой лекции можно посмотреть запись лекции 2021.02.19 вместо двеннадцатой лекции можно посмотреть запись лекции 2021.02.24 вместо триннадцатой и четырнадцатой лекций можно посмотреть вот эти видео: часть 1, часть 2 вместо пятнадцатой лекции можно посмотреть запись лекции 2021.03.03 вместо шестнадцатой лекции можно посмотреть запись лекции 2021.03.05 и другую и третью, вместо шестнадцатой лекции можно посмотреть вот это видео. 1.1 Используемые пакеты packageVersion(&quot;tidyverse&quot;) ## [1] &#39;2.0.0&#39; packageVersion(&quot;fitdistrplus&quot;) ## [1] &#39;1.1.8&#39; packageVersion(&quot;mixtools&quot;) ## [1] &#39;2.0.0&#39; packageVersion(&quot;lme4&quot;) ## [1] &#39;1.1.31&#39; packageVersion(&quot;lmerTest&quot;) ## [1] &#39;3.1.3&#39; packageVersion(&quot;car&quot;) ## [1] &#39;3.1.1&#39; packageVersion(&quot;pscl&quot;) ## [1] &#39;1.5.5&#39; packageVersion(&quot;nnet&quot;) ## [1] &#39;7.3.18&#39; packageVersion(&quot;MASS&quot;) ## [1] &#39;7.3.58.2&#39; packageVersion(&quot;ggeffects&quot;) ## [1] &#39;1.2.0&#39; packageVersion(&quot;brms&quot;) ## [1] &#39;2.18.0&#39; packageVersion(&quot;tidybayes&quot;) ## [1] &#39;3.0.3&#39; 1.2 Домашние задания домашнее задание к лекции 23.01.2023: вспомните пожалуйста, условные вероятности, формулу Байеса и при каких условиях ее применяют; посмотрите освежающие материалы про условную вероятность и формулу Байеса. домашнее задание 1. (дедлайны: 31.01.2023 7:59, 07.02.2023 7:59) домашнее задание 2. (дедлайны: 21.02.2023 7:59, 04.03.2023 7:59) домашнее задание 3. (дедлайны: 14.03.2023 7:59, 21.03.2023 7:59) "],["распределения.html", "2 Распределения 2.1 Распределения в R 2.2 Дискретные переменные 2.3 Числовые переменные", " 2 Распределения library(tidyverse) 2.1 Распределения в R В R встроено какое-то количество известных распределений. Все они представлены четырьмя функциями: d... (функция плотности, probability density function), p... (функция распределения, cumulative distribution function) — интеграл площади под кривой от начала до указанной квантили q... (обратная функции распределения, inverse cumulative distribution function) — значение p-той квантили распределения и r... (рандомные числа из заданного распределения). Рассмотрим все это на примере нормального распределения. tibble(x = 1:100, PDF = dnorm(x = x, mean = 50, sd = 10)) %&gt;% ggplot(aes(x, PDF))+ geom_point()+ geom_line()+ labs(title = &quot;PDF нормального распределения (μ = 50, sd = 10)&quot;) tibble(x = 1:100, CDF = pnorm(x, mean = 50, sd = 10)) %&gt;% ggplot(aes(x, CDF))+ geom_point()+ geom_line()+ labs(title = &quot;CDF нормального распределения (μ = 50, sd = 10)&quot;) tibble(quantiles = seq(0, 1, by = 0.01), value = qnorm(quantiles, mean = 50, sd = 10)) %&gt;% ggplot(aes(quantiles, value))+ geom_point()+ geom_line()+ labs(title = &quot;inverse CDF нормального распределения (μ = 50, sd = 10)&quot;) tibble(sample = rnorm(100, mean = 50, sd = 10)) %&gt;% ggplot(aes(sample))+ geom_histogram()+ labs(title = &quot;выборка нормально распределенных чисел (μ = 50, sd = 10)&quot;) Если не использовать set.seed(), то результат работы рандомизатора нельзя будет повторить. Какое значение имеет 25% квантиль нормального распределения со средним в 20 и стандартным отклонением 90? Ответ округлите до трех знаков после запятой. Данные из базы данных фонетических инвентарей PHOIBLE (Moran, McCloy, and Wright 2014), достаточно сильно упрощая, можно описать нормальным распределением со средним 35 фонем и стандартным отклонением 13. Если мы ничего не знаем про язык, оцените с какой вероятностью, согласно этой модели произвольно взятый язык окажется в промежутке между 25 и 50 фонемами? Ответ округлите до трех знаков после запятой. Какие есть недостатки у модели из предыдущего задания? ответы: 2.2 Дискретные переменные 2.2.1 Биномиальное распределение Биномиальное распределение — распределение количетсва успехов эксперементов Бернулли из n попыток с вероятностью успеха p. \\[P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} = {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\] \\[ 0 \\leq p \\leq 1; n, k &gt; 0\\] tibble(x = 0:50, density = dbinom(x = x, size = 50, prob = 0.16)) %&gt;% ggplot(aes(x, density))+ geom_point()+ geom_line()+ labs(title = &quot;Биномиальное распределение p = 0.16, n = 50&quot;) Немного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%). Какова вероятность, согласно этим данным, что в интервью британского актера из 118 контекстов будет 102 s-генитивов? Ответ округлите до трёх ИЛИ МЕНЕЕ знаков после запятой. А какое значение количества s-генитивов наиболее ожидаемо, согласно этой модели? 2.2.2 Геометрическое распределение Геометрическое распределение — распределение количетсва эксперементов Бернулли с вероятностью успеха p до первого успеха. \\[P(k | p) = (1-p)^k\\times p\\] \\[k\\in\\{1, 2, \\dots\\}\\] tibble(x = 0:50, density = dgeom(x = x, prob = 0.16)) %&gt;% ggplot(aes(x, density))+ geom_point()+ geom_line()+ labs(title = &quot;Геометрическое распределение p = 0.16, n = 50&quot;) Приняв модель из (Rosenbach 2003: 394), какова вероятность, что в интервью с британским актером первый of-генитив будет третьим по счету? 2.2.3 Распределение Пуассона Распределение дискретной переменной, обозначающей количество случаев \\(k\\) некоторого события, которое происходит с некоторой заданной частотой \\(\\lambda\\). \\[P(\\lambda) = \\frac{e^{-\\lambda}\\times\\lambda^k}{k!}\\] tibble(k = 0:50, density = dpois(x = k, lambda = 5)) %&gt;% ggplot(aes(k, density))+ geom_point()+ geom_line()+ labs(title = &quot;Распределение Пуассона с параметром λ = 5&quot;) Параметр \\(\\lambda\\) в модели Пуассона одновременно является и средним, и дисперсией. Попробуем воспользоваться распределением Пуассона для моделирования количества слогов в андийском языке. Количество слогов – это всегда натуральное число (т. е. не бывает 2.5 слогов, не бывает -3 слогов и т. д., но в теории может быть 0 слогов), так что модель Пуассона здесь применима. Согласно модели Пуассона все слова независимо друг от друга получают сколько-то слогов согласно распределению Пуассона. Посмотрим на данные: andic_syllables &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/andic_syllables.csv&quot;) andic_syllables %&gt;% ggplot(aes(n_syllables, count))+ geom_col()+ facet_wrap(~language, scales = &quot;free&quot;) Птичка напела (мы научимся узнавать, откуда птичка это знает на следующем занятии), что андийские данные можно описать при помощи распределения Пуассона с параметром \\(\\lambda\\) = 2.783. andic_syllables %&gt;% filter(language == &quot;Andi&quot;) %&gt;% rename(observed = count) %&gt;% mutate(predicted = dpois(n_syllables, lambda = 2.783)*sum(observed)) %&gt;% pivot_longer(names_to = &quot;type&quot;, values_to = &quot;value&quot;, cols = c(observed, predicted)) %&gt;% ggplot(aes(n_syllables, value, fill = type))+ geom_col(position = &quot;dodge&quot;) На графиках ниже представлены предсказания трех Пуассоновских моделей, какая кажется лучше? Выше было написано: Согласно модели Пуассона все слова независимо друг от друга получают сколько-то слогов согласно распределению Пуассона. Какие проблемы есть у предположения о независимости друг от друга количества слогов разных слов в словаре? 2.3 Числовые переменные 2.3.1 Нормальное распределение \\[P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\times e^{-\\frac{\\left(x-\\mu\\right)^2}{2\\sigma^2}}\\] \\[\\mu \\in \\mathbb{R}; \\sigma^2 &gt; 0\\] tibble(x = 1:100, PDF = dnorm(x = x, mean = 50, sd = 10)) %&gt;% ggplot(aes(x, PDF))+ geom_point()+ geom_line()+ labs(title = &quot;PDF нормального распределения (μ = 50, sd = 10)&quot;) Птичка напела, что длительность гласных американского английского из (Hillenbrand et al. 1995) можно описать нормальным распределением с параметрами \\(\\mu =\\) 274.673 и \\(\\sigma =\\) 64.482. Посмотрим, как можно совместить данные и это распределение: vowels &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/phonTools_hillenbrand_1995.csv&quot;) vowels %&gt;% ggplot(aes(dur)) + geom_histogram(aes(y = after_stat(density))) + # обратите внимание на аргумент after_stat(density) stat_function(fun = dnorm, args = list(mean = 274.673, sd = 64.482), color = &quot;red&quot;) 2.3.2 Логнормальное распределение \\[P(x) = \\frac{1}{\\sqrt{x\\sigma2\\pi}}\\times e^{-\\frac{\\left(\\ln(x)-\\mu\\right)^2}{2\\sigma^2}}\\] \\[\\mu \\in \\mathbb{R}; \\sigma^2 &gt; 0\\] tibble(x = 1:100, PDF = dlnorm(x = x, mean = 3, sd = 0.5)) %&gt;% ggplot(aes(x, PDF))+ geom_point()+ geom_line()+ labs(title = &quot;PDF логнормального распределения (μ = 3, σ = 0.5)&quot;) Какая из логнормальных моделей для длительности гласных американского английского из (Hillenbrand et al. 1995) лучше подходит к данным? Попробуйте самостоятельно построить данный график. 2.3.3 Что еще почитать про распределения? Люди придумали очень много разных распределений. Стоит, наверное, также понимать, что распределения не существуют отдельно в вакууме: многие из них математически связаны друг с другом. Про это можно посмотреть вот здесь или здесь. Ссылки на литературу "],["метод-максимального-правдоподобия.html", "3 Метод максимального правдоподобия 3.1 Оценка вероятности 3.2 Функция правдоподобия 3.3 Пример с непрерывным распределением 3.4 Метод максимального правдоподобия (MLE) 3.5 Логорифм функции правдоподобия", " 3 Метод максимального правдоподобия 3.1 Оценка вероятности library(tidyverse) Когда у нас задано некоторое распределение, мы можем задавать к нему разные вопросы. Например, если мы верим что длительность гласных американского английского из (Hillenbrand et al. 1995) можно описать логнормальным распределением с параметрами \\(\\ln{\\mu} =\\) 5.587 и \\(\\ln{\\sigma} =\\) 0.242, то мы можем делать некотрые предсказания относительно интересующей нас переменной. ggplot() + stat_function(fun = dlnorm, args = list(mean = 5.587, sd = 0.242))+ scale_x_continuous(breaks = 0:6*100, limits = c(0, 650))+ labs(x = &quot;длительность гласного (мс)&quot;, y = &quot;значение функции плотности&quot;) Если принять на веру, что логнормальное распределение с параметрами \\(\\ln{\\mu} =\\) 5.587 и \\(\\ln{\\sigma}=\\) 0.242 описывает данные длительности гласных американского английского из (Hillenbrand et al. 1995), то какова вероятность наблюдать значения между 300 и 400 мс? То же самое можно записать, используя математическую нотацию: \\[P\\left(X \\in [300,\\, 400] | X \\sim \\ln{\\mathcal{N}}(\\ln{\\mu} = 5.587, \\ln{\\sigma}=0.242)\\right) = ??\\] Ответ округлите до трех и меньше знаков после запятой. Если принять на веру, что биномиальное распределение с параметрами \\(p =\\) 0.9 описывает, согласно (Rosenbach 2003: 394) употребление s-генитивов в британском английском, то какова вероятность наблюдать значения между 300 и 350 генитивов в интервью, содержащее 400 генитивных контекстов? То же самое можно записать, используя математическую нотацию: \\[P\\left(X \\in [300,\\, 350] | X \\sim Binom(n = 400, p = 0.9)\\right) = ??\\] Ответ округлите до трех и меньше знаков после запятой. 3.2 Функция правдоподобия Если при поиске вероятностей, мы предполагали, что данные нам неизвестны, а распределение и его параметры известны, то функция правдоподобия позволяет этот процесс перевернуть, запустив поиск параметров распределения, при изветсных данных и семье распределения: \\[L\\left(X \\sim Distr(...)|x\\right) = ...\\] Таким образом получается, что на основании функции плотности мы можем сравнивать, какой параметр лучше подходит к нашим данным. Для примера рассмотрим наш s-генетив: мы провели интервью и нам встретилось 85 s-генетивов из 100 случаев всех генетивов. Насколько хорошо подходит нам распределение с параметром p = 0.9? Ответ: dbinom(85, 100, 0.9) [1] 0.03268244 Представим теперь это как функцию от параметра p: tibble(p = seq(0, 1, by = 0.01)) %&gt;% ggplot(aes(p)) + stat_function(fun = function(p) dbinom(85, 100, p), geom = &quot;col&quot;)+ labs(x = &quot;параметр биномиального распределения p&quot;, y = &quot;значение функции правдоподобия\\n(одно наблюдение)&quot;) А что если мы располагаем двумя интервью одного актера? В первом на сто генитивов пришлось 85 s-генитивов, а во втором – 89. В таком случае, также как и с вероятностью наступления двух независимых событий, значения функции плотности перемножаются. dbinom(85, 100, 0.9)*dbinom(89, 100, 0.9) [1] 0.003917892 tibble(p = seq(0, 1, by = 0.01)) %&gt;% ggplot(aes(p)) + stat_function(fun = function(p) dbinom(85, 100, p)*dbinom(89, 100, p), geom = &quot;col&quot;)+ labs(x = &quot;параметр биномиального распределения p&quot;, y = &quot;значение функции правдоподобия\\n(два наблюдения)&quot;) В итоге: вероятность — P(data|distribution) правдоподобие — L(distribution|data) Интеграл распределения/сумма значений вероятностей равен/на 1. Интеграл распределения/сумма значений правдоподобия может быть не равен/на 1. 3.3 Пример с непрерывным распределением Мы уже обсуждали, что длительность гласных американского английского из (Hillenbrand et al. 1995) можно описать логнормальным распределением с параметрами \\(\\ln\\mu\\) и \\(\\ln\\sigma\\). Предположим, что \\(\\ln\\sigma = 0.342\\), построим функцию правдоподобия для \\(\\ln\\mu\\): vowels &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/phonTools_hillenbrand_1995.csv&quot;) tibble(ln_mu = seq(5, 6, by = 0.001)) %&gt;% ggplot(aes(ln_mu)) + stat_function(fun = function(ln_mu) dlnorm(vowels$dur[1], meanlog = ln_mu, sdlog = 0.242))+ labs(x = &quot;параметр логнормального распределения ln μ&quot;, y = &quot;значение функции правдоподобия\\n(одно наблюдение)&quot;) tibble(ln_mu = seq(5, 6, by = 0.001)) %&gt;% ggplot(aes(ln_mu)) + stat_function(fun = function(ln_mu) dlnorm(vowels$dur[1], meanlog = ln_mu, sdlog = 0.242)*dlnorm(vowels$dur[2], meanlog = ln_mu, sdlog = 0.242))+ labs(x = &quot;параметр логнормального распределения ln μ&quot;, y = &quot;значение функции правдоподобия\\n(два наблюдения)&quot;) tibble(ln_mu = seq(5, 6, by = 0.001)) %&gt;% ggplot(aes(ln_mu)) + stat_function(fun = function(ln_mu) dlnorm(vowels$dur[1], meanlog = ln_mu, sdlog = 0.242)*dlnorm(vowels$dur[2], meanlog = ln_mu, sdlog = 0.242)*dlnorm(vowels$dur[3], meanlog = ln_mu, sdlog = 0.242))+ labs(x = &quot;параметр логнормального распределения ln μ&quot;, y = &quot;значение функции правдоподобия\\n(три наблюдения)&quot;) Для простоты в начале я зафиксировал один из параметров логнормального распредления: лог стандартное отклонение. Конечно, это совсем необязательно делать: можно создать матрицу значений лог среднего и лог стандартного отклонения и получить для каждой ячейки матрицы значения функции правдоподобия. 3.4 Метод максимального правдоподобия (MLE) Функция правдоподобия позволяет подбирать параметры распределения. Оценка параметров распределения при помощи функции максимального правдоподобия получила название метод максимального правдоподобия. Его я и использовал ранее для того, чтобы получить значения распределений для заданий из первого занятия: данные длительности американских гласных из (Hillenbrand et al. 1995) и логнормальное распределение fitdistrplus::fitdist(vowels$dur, distr = &#39;lnorm&#39;, method = &#39;mle&#39;) Fitting of the distribution &#39; lnorm &#39; by maximum likelihood Parameters: estimate Std. Error meanlog 5.5870359 0.005935135 sdlog 0.2423978 0.004196453 количество андийских слогов в словах и распределение Пуассона andic_syllables &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/andic_syllables.csv&quot;) andic_syllables %&gt;% filter(language == &quot;Andi&quot;) %&gt;% uncount(count) %&gt;% pull(n_syllables) %&gt;% fitdistrplus::fitdist(distr = &#39;pois&#39;, method = &#39;mle&#39;) Fitting of the distribution &#39; pois &#39; by maximum likelihood Parameters: estimate Std. Error lambda 2.782715 0.02128182 Есть и другие методы оценки параметров. Метод максимального правдоподобия может быть чувствителен к размеру выборки. Отфильтруйте из данных с количеством слогов в андийских языках багвалинский и, используя метод максимального правдоподобия, оцените для них параметры модели Пуассона. В работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, оставив односложные слова (переменная syllables) после придыхательного (переменная aspiration), произнесенные носителем tt01 (переменная speaker) и постройте следующий график, моделируя длительность гласных (переменная vowel.dur) нормальным и логнормальным распределением. Как вам кажется, какое распределение лучше подходит к данным? Докажите ваше утверждение, сравнив значения правдоподобия. 3.5 Логорифм функции правдоподобия Так как в большинстве случаев нужно найти лишь максимум функции правдоподобия, а не саму функцию \\(\\ell(x|\\theta)\\), то для облегчения подсчетов используют логорифмическую функцию правдоподобия \\(\\ln\\ell(x|\\theta)\\): в результате, вместо произведения появляется сумма1: \\[\\text{argmax}_\\theta \\prod \\ell(\\theta|x) = \\text{argmax}_\\theta \\sum \\ln\\ell(\\theta|x) \\] Во всех предыдущих примерах мы смотрели на 1-3 примера данных, давайте попробуем использовать функцию правдоподобия для большего набора данных. Представим, что мы проводим некоторый эксперимент, и у некоторых участников все получается с первой попытки, а некоторым нужна еще одна попытка или даже две. Дополните код функциями правдоподобия и логорифмической функцией правдоподобия, чтобы получился график ниже. set.seed(42) v &lt;- sample(0:2, 10, replace = TRUE) sapply(seq(0.01, 0.99, 0.01), function(p){ ... }) -&gt; likelihood sapply(seq(0.01, 0.99, 0.01), function(p){ ... }) -&gt; loglikelihood tibble(p = seq(0.01, 0.99, 0.01), loglikelihood, likelihood) %&gt;% pivot_longer(names_to = &quot;type&quot;, values_to = &quot;value&quot;, loglikelihood:likelihood) %&gt;% ggplot(aes(p, value))+ geom_line()+ geom_vline(xintercept = 0.33, linetype = 2)+ facet_wrap(~type, scales = &quot;free_y&quot;, nrow = 2)+ scale_x_continuous(breaks = c(0:5*0.25, 0.33)) Ссылки на литературу "],["модели-смеси-распределений.html", "4 Модели смеси распределений 4.1 Cмеси распределений 4.2 Модели смеси распределений 4.3 Несколько замечаний", " 4 Модели смеси распределений 4.1 Cмеси распределений Не все переменные выглядят так же красиво, как распределения из учебников статистики. Для примера возьмем датасет, который содержит спамерские и обычные смс-сообщения, выложенный UCI Machine Learning на kaggle. Посчитаем количество символов в сообщениях: spam_sms &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/spam_sms.csv&quot;) glimpse(spam_sms) Rows: 5,572 Columns: 2 $ type &lt;chr&gt; &quot;ham&quot;, &quot;ham&quot;, &quot;spam&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;spam&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;spa… $ message &lt;chr&gt; &quot;Go until jurong point, crazy.. Available only in bugis n grea… spam_sms %&gt;% mutate(n_char = nchar(message)) -&gt; spam_sms glimpse(spam_sms) Rows: 5,572 Columns: 3 $ type &lt;chr&gt; &quot;ham&quot;, &quot;ham&quot;, &quot;spam&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;spam&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;spa… $ message &lt;chr&gt; &quot;Go until jurong point, crazy.. Available only in bugis n grea… $ n_char &lt;int&gt; 111, 29, 155, 49, 61, 147, 77, 160, 157, 154, 109, 136, 155, 1… spam_sms %&gt;% ggplot(aes(n_char))+ geom_histogram(fill = &quot;gray90&quot;)+ labs(caption = &quot;данные из kaggle.com/uciml/sms-spam-collection-dataset&quot;, x = &quot;количество символов&quot;, y = &quot;значение функции плотности&quot;) Мы видим два явных горба и, как можно догадаться, это связано с тем, что спамерские сообщения в среднем длиннее и сосредоточены вокруг ограничения смс в 160 символов: spam_sms %&gt;% ggplot(aes(n_char))+ geom_histogram(fill = &quot;gray70&quot;, aes(y = after_stat(density)))+ geom_density(aes(fill = type), alpha = 0.3)+ labs(caption = &quot;данные из kaggle.com/uciml/sms-spam-collection-dataset&quot;, x = &quot;количество символов&quot;, y = &quot;значение функции плотности&quot;)+ geom_vline(xintercept = 160, linetype = 2, size = 0.3) 4.2 Модели смеси распределений Такого рода данные можно описать при помощи модели смеси разных распределений. Мы сейчас опишем нормальными распределениями и будем использовать пакет mixtools (для смесей нормальных распределений лучше использовать пакет mclust), но, ясно, что семейство распределений можно было бы подобрать и получше. library(mixtools) set.seed(42) spam_length_est &lt;- normalmixEM(spam_sms$n_char) number of iterations= 73 summary(spam_length_est) summary of normalmixEM object: comp 1 comp 2 lambda 0.439334 0.560666 mu 37.858905 114.070490 sigma 13.398985 60.921536 loglik at estimate: -29421.36 Класс, получаемый в результате работы функции normalmixEM() имеет встроеный график: plot(spam_length_est, density = TRUE) Однако, если хочется больше контроля над получаемым разультатом, я бы предложил использовать ggplot(): new_dnorm &lt;- function(x, mu, sigma, lambda){ dnorm(x, mu, sigma)*lambda } spam_sms %&gt;% ggplot(aes(n_char))+ geom_histogram(aes(y = after_stat(density)), fill = &quot;gray90&quot;)+ stat_function(fun = new_dnorm, args = c(mu = spam_length_est$mu[1], sigma = spam_length_est$sigma[1], lambda = spam_length_est$lambda[1]), color = &quot;#F8766D&quot;)+ stat_function(fun = new_dnorm, args = c(mu = spam_length_est$mu[2], sigma = spam_length_est$sigma[2], lambda = spam_length_est$lambda[2]), color = &quot;#00BFC4&quot;)+ labs(caption = &quot;данные из kaggle.com/uciml/sms-spam-collection-dataset&quot;, x = &quot;количество символов&quot;, y = &quot;значение функции плотности&quot;)+ geom_vline(xintercept = 160, linetype = 2, size = 0.3) Таким образом мы получили классификатор first &lt;- new_dnorm(seq(1, 750, by = 1), mu = spam_length_est$mu[1], sigma = spam_length_est$sigma[1], lambda = spam_length_est$lambda[1]) second &lt;- new_dnorm(seq(1, 750, by = 1), mu = spam_length_est$mu[2], sigma = spam_length_est$sigma[2], lambda = spam_length_est$lambda[2]) which(first &gt; second) [1] 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 [26] 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 [51] 56 57 58 59 60 61 62 Если в смс-сообщении больше 62 символов, то согласно нашей модели, вероятнее всего это спам. spam_sms %&gt;% mutate(model_predict = ifelse(n_char &gt; 63, &quot;predicted_spam&quot;, &quot;predicted_ham&quot;)) %&gt;% count(model_predict, type) %&gt;% pivot_wider(names_from = type, values_from = n) Результат не идеальный, но лучше чем помечать как спам каждое 13 сообщение (\\(747/(4825+747)\\)). В работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, оставив наблюдения гласного [a] (переменная vowel), произнесенные носителем tt01 (переменная speaker) и постройте следующие графики, моделируя длительность гласного (переменная vowel.dur) смесью трех нормальных распределений. Как вам кажется, насколько хорошо модель смеси справилась с заданием? number of iterations= 114 4.3 Несколько замечаний В наших примерах нам была доступна информация о классах (spam/ham, coronal/labial/velar), однако модель смесей распределений как раз имеет смысл применять, когда такой информации нет. В смеси распределений может быть любое количество распределений. Модели смеси распределений не ограничены только нормальным распределением, алгоритм можно использовать и для других распределений. Чаще всего в моделях смеси распределений используются распределения одного семейства, однако можно себе представить и комбинации посложнее. Модели смеси распределений (mixture models) не стоит путать со смешанными моделями (mixed effects models). w Ссылки на литературу "],["байесовский-статистический-вывод.html", "5 Байесовский статистический вывод 5.1 Нотация 5.2 Категориальный пример 5.3 Разница между фриквентиским и байесовским подходами 5.4 Биномиальные данные 5.5 Байесовский апдейт нормального распределения 5.6 Другие распределения 5.7 Вопросы к апостериорному распределению", " 5 Байесовский статистический вывод 5.1 Нотация В байесовском подходе статистический вывод описывается формулой Байеса \\[P(θ|Data) = \\frac{P(Data|θ)\\times P(θ)}{P(Data)}\\] \\(P(θ|Data)\\) — апостериорная вероятность (posterior) \\(P(Data|θ)\\) — функция правдоподобия (likelihood) \\(P(θ)\\) — априорная вероятность (prior) \\(P(Data)\\) — нормализующий делитель В литературе можно еще встретить такую запись: \\[P(θ|Data) \\propto P(Data|θ)\\times P(θ)\\] На прошлых занятиях мы говорили, что функция правдоподобия не обязана интегрироваться до 1, тогда почему, назвав часть формулы Байеса \\(P(Data|θ)\\) функцией правдоподобия, мы оставляем нотацию, будто это функция вероятностей? Потому что это условная вероятность, она не обязана интегрироваться до 1. 5.2 Категориальный пример Для примера я взял датасет, который содержит спамерские и обычные смс-сообщения, выложенный UCI Machine Learning на kaggle и при помощи пакета udpipe токенизировал и определил часть речи: sms_pos &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/spam_sms_pos.csv&quot;) glimpse(sms_pos) Rows: 34 Columns: 3 $ type &lt;chr&gt; &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;h… $ upos &lt;chr&gt; &quot;ADJ&quot;, &quot;ADP&quot;, &quot;ADV&quot;, &quot;AUX&quot;, &quot;CCONJ&quot;, &quot;DET&quot;, &quot;INTJ&quot;, &quot;NOUN&quot;, &quot;NUM&quot;… $ n &lt;dbl&gt; 4329, 5004, 5832, 5707, 1607, 3493, 1676, 12842, 1293, 2424, 1144… sms_pos %&gt;% group_by(type) %&gt;% mutate(ratio = n/sum(n), upos = fct_reorder(upos, n, mean, .desc = TRUE)) %&gt;% ggplot(aes(type, ratio))+ geom_col()+ geom_label(aes(label = round(ratio, 3)), position = position_stack(vjust = 0.5))+ facet_wrap(~upos, scales = &quot;free_y&quot;) Давайте полученные доли считать нашей моделью: сумма всех чисел внутри каждого типа (ham/spam) дает в сумме 1. Мы получили новое сообщение: Call FREEPHONE 0800 542 0825 now! Модель udpipe разобрала его следующим образом: VERB NUM NUM NUM NUM ADV PUNCT Понятно, что это – спам, но мы попытаемся применить байесовский статистический вывод, чтобы определить тип сообщения. Предположим, что машина считает обе гипотезы равновероятными, т. е. ее априорное распределение гипотез равно 0.5 каждая. На минуту представим, что машина анализирует текст пословно. Первое слово типа VERB. Функции правдоподобия равны 0.135 и 0.096 для сообщений типа ham и spam соответственно. Применим байесовский апдейт: tibble(model = c(&quot;ham&quot;, &quot;spam&quot;), prior = 0.5, likelihood = c(0.135, 0.096), product = prior*likelihood, posterior = product/sum(product)) Вот мы и сделали байесовский апдейт. Теперь апостериорное распределение, которое мы получили на предыдущем шаге, мы можем использовать в новом апдейте. Следующее слово в сообщении типа NUM. tibble(model = c(&quot;ham&quot;, &quot;spam&quot;), prior_2 = c(0.584, 0.416), likelihood_2 = c(0.016, 0.117), product_2 = prior_2*likelihood_2, posterior_2 = product_2/sum(product_2)) Уже на второй итерации, наша модель почти уверена, что это сообщение spam. На третьей итерации уверенность только растет: tibble(model = c(&quot;ham&quot;, &quot;spam&quot;), prior_3 = c(0.161, 0.839), likelihood_3 = c(0.016, 0.117), product_3 = prior_3*likelihood_3, posterior_3 = product_3/sum(product_3)) На основе первых трех слов посчитайте посчитайте вероятность гипотезы, что перед нами спамерское сообщение, если предположить, что каждое пятое сообщение – спам. Ответ округлите до трех знаков после запятой. Из формулы Байеса следует, что не обязательно каждый раз делить на нормализующий делитель, это можно сделать единожды. tibble(model = c(&quot;ham&quot;, &quot;spam&quot;), prior = 0.5, likelihood = c(0.135, 0.096), likelihood_2 = c(0.016, 0.117), product = prior*likelihood*likelihood_2*likelihood_2, posterior = product/sum(product)) Из приведенных рассуждений также следует, что все равно в каком порядке мы производим байесовский апдейт: мы могли сначала умножить на значение правдоподобия для категории NUM и лишь в конце на значение правдоподобия VERB. Также стоит отметить, что если данных много, то через какое-то время становится все равно, какое у нас было априорное распределение. Даже в нашем примере, в котором мы проанализировали первые три слова сообщения, модель, прогнозирующая, что сообщение спамерское, выиграет, даже если, согласно априорному распределению, спамерским является каждое 20 сообщение: tibble(model = c(&quot;ham&quot;, &quot;spam&quot;), prior = c(0.95, 0.05), likelihood = c(0.135, 0.096), likelihood_2 = c(0.016, 0.117), product = prior*likelihood*likelihood_2*likelihood_2, posterior = product/sum(product)) Самым главным отличием байесовского статистического вывода от фриквентистского, является то, что мы в результате получаем вероятность каждой из моделей. Это очень значительно отличается от фриквентистской практики нулевых гипотез и p-value, в соответствии с которыми мы можем лишь отвергнуть или не отвергнуть нулевую гипотезу. Вашего друга похитили а на почту отправили датасет, в котором записаны данные о погоде из пяти городов. Ваш телефон зазвонил, и друг сказал, что не знает куда его похитили, но за окном легкий дождь (Rain). А в какой-то из следующих дней — сильный дождь (Rain, Thunderstorm). Исходя из явно неверного предположения, что погодные условия каждый день не зависят друг от друга, сделайте байесовский апдейт и предположите, в какой город вероятнее всего похитили друга. Auckland Beijing Chicago Mumbai San Diego Укажите получившуюся вероятность. Выполняя задание, округлите все вероятности и значения правдоподобия до 3 знаков после запятой. 5.3 Разница между фриквентиским и байесовским подходами Картинка из одной из моих любимых книг по статистике (Efron and Hastie 2016: 34). 5.4 Биномиальные данные Биномиальные данные возникают, когда нас интересует доля успехов в какой-то серии эксперементов Бернулли. 5.4.1 Биномиальное распределение Биномиальное распределение — распределение количества успехов эксперементов Бернулли из n попыток с вероятностью успеха p. \\[P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} = {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\] \\[ 0 \\leq p \\leq 1; n, k &gt; 0\\] tibble(x = 0:50, density = dbinom(x = x, size = 50, prob = 0.16)) %&gt;% ggplot(aes(x, density))+ geom_point()+ geom_line()+ labs(title = &quot;Биномиальное распределение p = 0.16, n = 50&quot;) 5.4.2 Бета распределение \\[P(x; α, β) = \\frac{x^{α-1}\\times (1-x)^{β-1}}{B(α, β)}; 0 \\leq x \\leq 1; α, β &gt; 0\\] Бета функция: \\[Β(α, β) = \\frac{Γ(α)\\times Γ(β)}{Γ(α+β)} = \\frac{(α-1)!(β-1)!}{(α+β-1)!} \\] tibble(x = seq(0, 1, length.out = 100), density = dbeta(x = x, shape1 = 8, shape2 = 42)) %&gt;% ggplot(aes(x, density))+ geom_point()+ geom_line()+ labs(title = &quot;Бета распределение α = 8, β = 42&quot;) Можно поиграть с разными параметрами: shiny::runGitHub(&quot;agricolamz/beta_distribution_shiny&quot;) \\[\\mu = \\frac{\\alpha}{\\alpha+\\beta}\\] \\[\\sigma^2 = \\frac{\\alpha\\times\\beta}{(\\alpha+\\beta)^2\\times(\\alpha+\\beta+1)}\\] 5.4.3 Байесовский апдейт биномиальных данных \\[Beta_{post}(\\alpha_{post}, \\beta_{post}) = Beta(\\alpha_{prior}+\\alpha_{data}, \\beta_{prior}+\\beta_{data}),\\] где \\(Beta\\) — это бета распределение shiny::runGitHub(&quot;agricolamz/bayes_for_binomial_app&quot;) Немного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%). Проведите байесовский апдейт, если Вы наблюдаете в интервью британского актера из 120 контекстов 92 s-генитивов. Априорное распределение берите соразмерное данным. Ответ округлите до трёх или менее знаков после запятой. Параметр альфа: Параметр бета: 5.4.4 Байесовский апдейт биномиальных данных: несколько моделей tibble(x = rep(seq(0, 1, length.out = 100), 6), density = c(dbeta(unique(x), shape1 = 8, shape2 = 42), dbeta(unique(x), shape1 = 16, shape2 = 34), dbeta(unique(x), shape1 = 24, shape2 = 26), dbeta(unique(x), shape1 = 8+4, shape2 = 42+16), dbeta(unique(x), shape1 = 16+4, shape2 = 34+16), dbeta(unique(x), shape1 = 24+4, shape2 = 26+16)), type = rep(c(&quot;prior&quot;, &quot;prior&quot;, &quot;prior&quot;, &quot;posterior&quot;, &quot;posterior&quot;, &quot;posterior&quot;), each = 100), dataset = rep(c(&quot;prior: 8, 42&quot;, &quot;prior: 16, 34&quot;, &quot;prior: 24, 26&quot;, &quot;prior: 8, 42&quot;, &quot;prior: 16, 34&quot;, &quot;prior: 24, 26&quot;), each = 100)) %&gt;% ggplot(aes(x, density, color = type))+ geom_line()+ facet_wrap(~dataset)+ labs(title = &quot;data = 4, 16&quot;) 5.4.5 Что почитать? Если остались неясности, то можно посмотреть 2-ую главу (Robinson 2017). 5.5 Байесовский апдейт нормального распределения Встроенный датасет ChickWeight содержит вес цыплят (weight) в зависимости от типа диеты (Diet). Мы будем анализировать 20-дневных птенцов. ChickWeight %&gt;% filter(Time == 20) -&gt; chicks chicks %&gt;% ggplot(aes(weight))+ geom_density() Начнем с апостериорных параметров для наблюдений \\(x_1, ... x_n\\) со средним \\(\\mu_{data}\\) известной дисперсией \\(\\sigma_{known}^2\\) 5.5.1 Байесовский апдейт нормального распределения: выбор из нескольких моделей Мы можем рассматривать эту задачу как выбор между несколькими моделями с разными средними: tibble(x = rep(1:400, 6), density = c(dnorm(unique(x), mean = 125, sd = 70), dnorm(unique(x), mean = 150, sd = 70), dnorm(unique(x), mean = 175, sd = 70), dnorm(unique(x), mean = 200, sd = 70), dnorm(unique(x), mean = 225, sd = 70), dnorm(unique(x), mean = 250, sd = 70)), dataset = rep(1:6, each = 400)) %&gt;% ggplot(aes(x, density, color = factor(dataset)))+ geom_line() Дальше мы можем точно так же апдейтить, как мы делали раньше: tibble(mu = seq(125, 250, by = 25), prior = 1/6, likelihood = c(prod(dnorm(chicks$weight, mean = 125, sd = 70)), prod(dnorm(chicks$weight, mean = 150, sd = 70)), prod(dnorm(chicks$weight, mean = 175, sd = 70)), prod(dnorm(chicks$weight, mean = 200, sd = 70)), prod(dnorm(chicks$weight, mean = 225, sd = 70)), prod(dnorm(chicks$weight, mean = 250, sd = 70))), product = prior*likelihood, posterior = product/sum(product)) -&gt; results results results %&gt;% select(mu, prior, posterior) %&gt;% pivot_longer(names_to = &quot;type&quot;, values_to = &quot;probability&quot;, prior:posterior) %&gt;% ggplot(aes(mu, probability, color = type))+ geom_point()+ labs(title = &quot;изменение вероятностей для каждой из моделей&quot;, x = &quot;μ&quot;) 5.5.2 Байесовский апдейт нормального распределения: непрерывный вариант Во первых, нам понадобится некоторая мера, которая называется точность (precision): \\[\\tau = \\frac{1}{\\sigma^2}\\] \\[\\tau_{post} = \\tau_{prior} + \\tau_{data} \\Rightarrow \\sigma^2_{post} = \\frac{1}{\\tau_{post}}\\] \\[\\mu_{post} = \\frac{\\mu_{prior} \\times \\tau_{prior} + \\mu_{data} \\times \\tau_{data}}{\\tau_{post}}\\] Так что если нашим априорным распределением мы назовем нормальное распределение со средним около 180 и стандартным отклонением 90, то процесс байесовского апдейта будет выглядеть вот так: sd_prior &lt;- 90 sd_data &lt;- sd(chicks$weight) sd_post &lt;- 1/sqrt(1/sd_prior^2 + 1/sd_data^2) mean_prior &lt;- 180 mean_data &lt;- mean(chicks$weight) mean_post &lt;- weighted.mean(c(mean_prior, mean_data), c(1/sd_prior^2, 1/sd_data^2)) chicks %&gt;% ggplot(aes(weight)) + geom_histogram(aes(y = after_stat(density)))+ stat_function(fun = dnorm, args = list(mean_prior, sd_prior), color = &quot;lightblue&quot;)+ stat_function(fun = dnorm, args = list(mean_post, sd_post), color = &quot;red&quot;) shiny::runGitHub(&quot;agricolamz/bayes_for_normal_app&quot;) В работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, произнесенные носителем tt01 (переменная speaker), произведите байесовский апдейт данных, моделируя длительность гласных (переменная vowel.dur) нормальным распределением и постройте график. В качестве априорного распределения используйте нормальное распределение со средним 87 и стандартным отклонением 25. 5.5.3 Что почитать? Murphy K. P. (2007) Conjugate Bayesian analysis of the Gaussian distribution Jordan M. I. (2010) The Conjugate Prior for the Normal Distribution раздел 2.5 в Gelman A. et. al (2014) Bayesian Data Analysis 5.6 Другие распределения Мы обсудили биномиальные и нормальнораспределенные данные. Так случилось, что для них есть короткий путь сделать байесовский апдейт, не применяя формулы байеса. И нам так повезло, что связки априорного/апосториорного распределений и функции правдоподобия такие простые: априорного/апосториорного распределены как бета распределение, значит функция правдоподобия – биномиальное распределение если мы моделируем данные при помощи нормального распределения, то все три распределения (априорное, функция правдопдобия и апосториорное) – нормальные. Такие отношения между распределениями называют сопряженными (conjugate). В результате для разных семейств функции правдоподобия существует список соответствующих сопряженных априорных распределений (conjugate prior), который можно найти, например, здесь. В более случаях используется (а на самом деле почти всегда) Марковские цепи Монте-Карло (MCMC). 5.7 Вопросы к апостериорному распределению A frequentist uses impeccable logic to answer the wrong question, while a Bayesian answers the right question by making assumptions that nobody can fully believe in. (P. G. Hammer) попытка оценить параметр θ и/или какой-нибудь интервал, в котором он лежит. среднее апостериорного распределения (mean of the posterior estimation, MAP) максимум апостериорного распределения (maximum a posteriori estimation, MAP) байесовский доверительный интервал ответить на вопросы вроде какова вероятность, что значение θ больше некоторого значения \\(x\\)? какова вероятность, что значение θ лежит в интервале \\([x; y]\\)? и т. п. Выборки из апостериорного распределения (Posterior simulation): симулируйте большую выборку из апостериорного распределения; используйте полученную выборку для статистического вывода. Допустим, мы получили апостериорное бета распределение с параметрами 20 и 70. Какова вероятность наблюдать значения больше 0.3? posterior_simulation &lt;- rbeta(n = 10000, shape1 = 20, shape2 = 70) sum(posterior_simulation &gt; 0.3)/10000 [1] 0.0457 И это не p-value! Это настоящие вероятности! Ссылки на литературу "],["байесовский-доверительный-интервал.html", "6 Байесовский доверительный интервал 6.1 Фреквентисткий доверительный интервал 6.2 Байесовский доверительный интервал", " 6 Байесовский доверительный интервал Рассмотрим простенькую задачу, которую мы видели раньше: Немного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%). Проведите байесовский апдейт, если Вы наблюдаете в интервью британского актера из 120 контекстов 92 s-генитивов. Априорное распределение берите соразмерное данным. Ответ округлите до трёх или менее знаков после запятой. tibble(x = seq(0, 1, by = 0.001), y = dbeta(x, 108+92, 12+28)) %&gt;% ggplot(aes(x, y))+ geom_line() 6.1 Фреквентисткий доверительный интервал Фреквентистский доверительный интервал (по-английски confidence interval) основан на правиле трех сигм нормального распределения: z-score: 95% данных находится в 1.96 стандартных отклонений 99% данных находится в 2.58 стандартных отклонений Доверительный интервал: предположим, что данные генеральной совокупности нормально распределены тогда доверительные интервалы выборок взятых из генеральной совокупности будут покрывать среднее генеральной совокупности \\[\\bar{x} \\pm z \\times \\frac{\\sigma}{\\sqrt{n}}\\text{, где } z \\text{ — это центральная } 1 - \\frac{\\alpha}{2} \\text{ часть данных}\\] Распространение этой логики на биномиальные данные называется интервал Вальда: \\[\\bar{x} = \\theta; \\sigma = \\sqrt{\\frac{\\theta\\times(1-\\theta)}{n}}\\] Тогда интервал Вальда: \\[\\theta \\pm z\\times\\sqrt{\\frac{\\theta\\times(1-\\theta)} {n}}\\] Есть только одна проблема: работает он плохо. Его аналоги перечислены в других работ: assymptotic method with continuity correction Wilson score Wilson Score method with continuity correction Jeffreys interval Clopper–Pearson interval (default in R binom.test()) Agresti–Coull interval … см. пакет binom low_ci &lt;- binom.test(x = 108+92, n = 108+92+12+28)$conf.int[1] up_ci &lt;- binom.test(x = 108+92, n = 108+92+12+28)$conf.int[2] tibble(x = seq(0, 1, by = 0.001), y = dbeta(x, 108+92, 12+28)) %&gt;% ggplot(aes(x, y))+ geom_line()+ annotate(geom = &quot;errorbar&quot;, y = 0, xmin = low_ci, xmax = up_ci, color = &quot;red&quot;)+ labs(title = &quot;Апостериорное распределение&quot;, subtitle = &quot;красным фреквентисткий 95% доверительный интервал&quot;, x = &quot;&quot;, y = &quot;&quot;) В базовом пакете функция binom.test() не позволяет выбирать тип доверительного интервала. ci.method = \"Clopper-Pearson\" возможна, если включить библиотеку mosaic. 6.2 Байесовский доверительный интервал Байесовский доверительный \\((100-k)\\)-% интервал (по-английски credible interval) — это интервал \\([\\frac{k}{2}, 1-\\frac{k}{2}]\\) от апостериорного распределения. low_ci &lt;- binom.test(x = 108+92, n = 108+92+12+28)$conf.int[1] up_ci &lt;- binom.test(x = 108+92, n = 108+92+12+28)$conf.int[2] cred_int_l &lt;- qbeta(0.025, 108+92, 12+28) cred_int_h &lt;- qbeta(0.975, 108+92, 12+28) tibble(x = seq(0, 1, by = 0.001), y = dbeta(x, 108+92, 12+28)) %&gt;% ggplot(aes(x, y))+ geom_line()+ annotate(geom = &quot;errorbar&quot;, y = 0, xmin = low_ci, xmax = up_ci, color = &quot;red&quot;)+ annotate(geom = &quot;errorbar&quot;, y = -1, xmin = cred_int_l, xmax = cred_int_h, color = &quot;lightblue&quot;)+ labs(title = &quot;Апостериорное распределение&quot;, subtitle = &quot;красным фреквентисткий 95% доверительный интервал\\nсиним байесовский 95% доверительный интервал&quot;, x = &quot;&quot;, y = &quot;&quot;) В работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, произнесенные носителем tt01 (переменная speaker), произведите байесовский апдейт данных, моделируя длительность гласных (переменная vowel.dur) нормальным распределением и постройте график. На графике отобразите 80% и 95% байесовский доверительный интервал (при построении интервала я использовал аргумент width = 0.001). В качестве априорного распределения используйте нормальное распределение со средним 87 и стандартным отклонением 25. Ссылки на литературу "],["коэффициент-байеса.html", "7 Коэффициент Байеса 7.1 Формула Байеса опять 7.2 Категориальные данные 7.3 Интерпретация коэфициента Байеса 7.4 Биномиальные данные", " 7 Коэффициент Байеса 7.1 Формула Байеса опять \\[P(\\theta|Data) = \\frac{P(Data|\\theta) \\times P(\\theta) }{P(Data)}\\] Рассмотрим какой-то простой случай, который мы уже видели много раз. Немного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%). Проведите байесовский апдейт, если Вы наблюдаете в интервью британского актера из 120 контекстов 92 s-генитивов. Априорное распределение берите соразмерное данным. Если мы не будем следовать простой дорожкой, которую мы обсуждали несколько разделов назад, а будем все делать согласно формуле Байеса, то получатся следующие компоненты: априорное распределение tibble(x = seq(0, 1, 0.001), prior = dbeta(x = x, shape1 = 120*0.9, shape2 = 120*0.1)) %&gt;% ggplot(aes(x, prior))+ geom_line(color = &quot;red&quot;) функция правдоподобия tibble(x = seq(0, 1, 0.001), likelihood = dbinom(x = 92, size = 120, prob = x)) %&gt;% ggplot(aes(x, likelihood))+ geom_line() их произведение (пропорционально апостериорному распределению) tibble(x = seq(0, 1, 0.001), prior = dbeta(x = x, shape1 = 120*0.9, shape2 = 120*0.1), likelihood = dbinom(x = 92, size = 120, prob = x), product = prior*likelihood) %&gt;% ggplot(aes(x, product))+ geom_line() предельное правдоподобие, которое позволяет сделать получившееся распределение распределением вероятностей marginal_likelihood &lt;- integrate(function(p){ dbinom(92, 120, p) * dbeta(p, 120*0.9, 120*0.1)}, lower = 0, upper = 1) marginal_likelihood 0.0009531395 with absolute error &lt; 0.000044 … и в результате получается апостериорное распределение! tibble(x = seq(0, 1, 0.001), prior = dbeta(x = x, shape1 = 120*0.9, shape2 = 120*0.1), likelihood = dbinom(x = 92, size = 120, prob = x), product = prior*likelihood, posterior = product/marginal_likelihood[[1]]) %&gt;% ggplot(aes(x, posterior))+ geom_line(color = &quot;darkgreen&quot;)+ geom_line(aes(y = prior), color = &quot;red&quot;) … которое мы умеем доставать и быстрее: tibble(x = seq(0, 1, 0.001), prior = dbeta(x = x, shape1 = 120*0.9, shape2 = 120*0.1), likelihood = dbinom(x = 92, size = 120, prob = x), product = prior*likelihood, posterior = product/marginal_likelihood[[1]], posterior_2 = dbeta(x = x, shape1 = 120*0.9+92, shape2 = 120*0.1+120-92)) %&gt;% ggplot(aes(x, posterior))+ geom_line(color = &quot;darkgreen&quot;, size = 2)+ geom_line(aes(y = prior), color = &quot;red&quot;)+ geom_line(aes(y = posterior_2), linetype = 2, color = &quot;yellow&quot;) Представим себе, что у нас есть \\(k\\) гипотез \\(M\\). Тогда формула Байеса может выглядеть вот так: \\[P(M_k|Data) = \\frac{P(Data|M_k) \\times P(M_k) }{P(Data)}\\] В данном занятии мы рассмотрим только случай двух модели, но можно рассматривать и случаи, когда моделей много. Посмотрим на соотношение апостериорных распределений двух моделей: \\[\\underbrace{\\frac{P(M_1 \\mid Data)}{P(M_2 \\mid Data)}}_{\\text{posterior odds}} = \\frac{\\frac{P(Data|M_1) \\times P(M_1) }{P(Data)}}{\\frac{P(Data|M_2) \\times P(M_2) }{P(Data)}}=\\underbrace{\\frac{P(Data \\mid M_1)}{P(Data \\mid M_2)}}_{\\text{Bayes factor}}\\times\\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\text{prior odds}}\\] Таким образом байесовский коэффициент это соотношение апосториорных распределений деленное на соотношение априорных распределений. \\[BF_{12}= \\frac{P(M_1 \\mid Data)/P(M_2 \\mid Data)}{P(M_1)/P(M_2)}=\\frac{P(M_1 \\mid Data)\\times P(M_2)}{P(M_2 \\mid Data)\\times P(M_1)}\\] В результате получается, что коэффициент Байеса – это соотношение предельных правдоподобий (знаменатель теоремы Байеса): \\[BF_{12}= \\frac{P(Data|\\theta, M_1))}{P(Data|\\theta, M_2))}=\\frac{\\int P(Data|\\theta, M_1)\\times P(\\theta|M_1)}{\\int P(Data|\\theta, M_2)\\times P(\\theta|M_2)}\\] Важно заметить, что если вероятности априорных моделей равны, то байесовский коэффициент равен просто соотношению функций правдоподобия. Надо отметить, что не все тепло относятся к сравнению моделей байесовским коэффициентом (см. Gelman, Rubin 1994). 7.2 Категориальные данные Для примера обратимся снова к датасету, который содержит спамерские и обычные смс-сообщения, выложенному UCI Machine Learning на kaggle, и при помощи пакета udpipe токенизируем и определим часть речи: sms_pos &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/spam_sms_pos.csv&quot;) glimpse(sms_pos) Rows: 34 Columns: 3 $ type &lt;chr&gt; &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;h… $ upos &lt;chr&gt; &quot;ADJ&quot;, &quot;ADP&quot;, &quot;ADV&quot;, &quot;AUX&quot;, &quot;CCONJ&quot;, &quot;DET&quot;, &quot;INTJ&quot;, &quot;NOUN&quot;, &quot;NUM&quot;… $ n &lt;dbl&gt; 4329, 5004, 5832, 5707, 1607, 3493, 1676, 12842, 1293, 2424, 1144… sms_pos %&gt;% group_by(type) %&gt;% mutate(ratio = n/sum(n), upos = fct_reorder(upos, n, mean, .desc = TRUE)) %&gt;% ggplot(aes(type, ratio))+ geom_col()+ geom_label(aes(label = round(ratio, 3)), position = position_stack(vjust = 0.5))+ facet_wrap(~upos, scales = &quot;free_y&quot;) Давайте полученные доли считать нашей моделью: сумма всех чисел внутри каждого типа (ham/spam) дает в сумме 1. Мы получили новое сообщение: Call FREEPHONE 0800 542 0825 now! Модель udpipe разобрала его следующим образом: VERB NUM NUM NUM NUM ADV PUNCT Если мы считаем наши модели равновероятными: first_update &lt;- tibble(model = c(&quot;ham&quot;, &quot;spam&quot;), prior = 0.5, likelihood = c(0.135, 0.096), product = prior*likelihood, marginal_likelihood = sum(product), posterior = product/marginal_likelihood) first_update Если же мы примем во внимание, что наши классы не равноправны, то сможем посчитать это нашим априорным распределением для моделей. sms_pos %&gt;% uncount(n) %&gt;% count(type) %&gt;% mutate(ratio = n/sum(n)) -&gt; class_ratio class_ratio second_update &lt;- tibble(model = c(&quot;ham&quot;, &quot;spam&quot;), prior = class_ratio$ratio, likelihood = c(0.135, 0.096), product = prior*likelihood, marginal_likelihood = sum(product), posterior = product/marginal_likelihood) second_update # Bayes factor second_update$marginal_likelihood[1]/first_update$marginal_likelihood[1] [1] 1.098469 7.3 Интерпретация коэфициента Байеса 7.4 Биномиальные данные Рассмотрим простенькую задачу, которую мы видели раньше: Немного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%), а носители американского английского предпочитают s-генитив (85%) of-генитиву (15%). Мы наблюдаем актера, который в интервью из 120 контекстов использует в 92 случаях s-генитивы. Сравните модели при помощи байесовского коэффициента. tibble(x = seq(0, 1, by = 0.001), y = dbeta(x, 120*0.9, 120*0.1), z = dbeta(x, 120*0.85, 120*0.15)) %&gt;% ggplot(aes(x, y))+ geom_line(color = &quot;red&quot;)+ geom_line(aes(y = z), color = &quot;lightblue&quot;)+ geom_vline(xintercept = 92/120, linetype = 2) m1 &lt;- function(p) dbinom(92, 120, p) * dbeta(p, 120*0.9, 120*0.1) m2 &lt;- function(p) dbinom(92, 120, p) * dbeta(p, 120*0.85, 120*0.15) integrate(m1, 0, 1)[[1]]/integrate(m2, 0, 1)[[1]] [1] 0.0672068 В работе (Coretta 2016) собраны данные длительности исландских гласных (столбец vowel.dur). Отфильтруйте данные, произнесенные носителем tt01 (переменная speaker), посчитайте байесовский коэффициент (\\(B_{12}\\)) для двух априорных моделей: нормального распределения со средним 87 и стандартным отклонением 25. (\\(m_1\\)) нормального распределения со средним 85 и стандартным отклонением 30. (\\(m_2\\)) Ответ округлите до трёх или менее знаков после запятой. Ссылки на литературу "],["эмпирическая-байесовская-оценка.html", "8 Эмпирическая байесовская оценка", " 8 Эмпирическая байесовская оценка library(tidyverse) Метод эмпирической байесовской оценки (Empirical Bayes estimation) — один из байесовских методов, в рамках которого: производят оценку априорного распределения вероятностей на основании имеющихся данных используют полученное априорное распределение для получение апостериорной оценки для каждого наблюдения Рассмотрим пример данных из статьи (Daniel et al. 2019), в которой аннализировались интервью с людьми из деревени Михалёвская и исследовался ряд консервативных и инновативных черт в их речи. mikhalevskaja &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/ustya_data.csv&quot;) glimpse(mikhalevskaja) Rows: 359 Columns: 7 $ speaker &lt;chr&gt; &quot;avm1922&quot;, &quot;ans1925&quot;, &quot;avt1928&quot;, &quot;egp1928&quot;, &quot;lpp1928&quot;, &quot;p… $ year &lt;dbl&gt; 1922, 1925, 1928, 1928, 1928, 1928, 1930, 1933, 1935, 194… $ gender &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f… $ conservative &lt;dbl&gt; 92, 56, 12, 33, 2, 83, 22, 22, 33, 60, 58, 11, 55, 8, 30,… $ innovative &lt;dbl&gt; 60, 70, 46, 127, 23, 127, 41, 88, 85, 103, 200, 73, 70, 5… $ total &lt;dbl&gt; 152, 126, 58, 160, 25, 210, 63, 110, 118, 163, 258, 84, 1… $ feature &lt;chr&gt; &quot;adj&quot;, &quot;adj&quot;, &quot;adj&quot;, &quot;adj&quot;, &quot;adj&quot;, &quot;adj&quot;, &quot;adj&quot;, &quot;adj&quot;, &quot;… Представим себе, что мы решили задаться целью найти наиболее диалектных носителей: library(tidytext) mikhalevskaja %&gt;% mutate(ratio = conservative/total, speaker = reorder_within(speaker, ratio, feature)) %&gt;% ggplot(aes(ratio, speaker, color = gender))+ geom_point()+ facet_wrap(~feature, scales = &quot;free&quot;)+ scale_y_reordered() Не очень легко это анализировать… Давайте выберем один признак – подъем a: консервативными считались формы [ꞌpʲetʲero], а инновативной – реализация [ꞌpʲatʲərə]. Посчитаем долю и отсортируем: mikhalevskaja %&gt;% filter(feature == &quot;a-e&quot;) %&gt;% mutate(ratio = conservative/total) %&gt;% arrange(desc(ratio)) В целом, всего в интервью встречается от 4 до 244 контекстов для реализации признака. Хотим ли мы верить, что lpp1928 с 29 наблюдениями диалектнее, чем mgb1949 с 104 наблюдениями, только на основании доли? mikhalevskaja %&gt;% filter(feature == &quot;a-e&quot;) %&gt;% mutate(ratio = conservative/total) %&gt;% ggplot(aes(ratio))+ geom_histogram() Мы можем провести байесовский апдейт, но для этого нам нужно априорное распределение. Трюк, который предлагает байесовская эмпирическая оценка заключается в том, что априорное распределение можно попробовать получить на основании данных: mikhalevskaja %&gt;% filter(feature == &quot;a-e&quot;) %&gt;% mutate(ratio = conservative/total) %&gt;% filter(ratio != 0, # оказывается fitdist плохо работает, когда много крайних точек ratio != 1) -&gt; for_beta_estimation beta_est &lt;- fitdistrplus::fitdist(for_beta_estimation$ratio, distr = &#39;beta&#39;, method = &#39;mle&#39;) beta_est Fitting of the distribution &#39; beta &#39; by maximum likelihood Parameters: estimate Std. Error shape1 1.590621 0.4127354 shape2 5.445977 1.5766951 Сделаем байесовский апдейт: mikhalevskaja %&gt;% filter(feature == &quot;a-e&quot;) %&gt;% mutate(alpha_prior = beta_est$estimate[1], beta_prior = beta_est$estimate[2], alpha_post = conservative+alpha_prior, beta_post = innovative+beta_prior, mean_post = alpha_post/(alpha_post+beta_post), ratio = conservative/total) %&gt;% ggplot(aes(ratio, mean_post, label = speaker, color = total))+ geom_hline(yintercept = beta_est$estimate[1]/sum(beta_est$estimate), linetype = 2)+ geom_point()+ ggrepel::geom_text_repel() Как видно, байесовская оценка не сильно отличается от старой оценки средним, однако таким образом мы можем видеть, что после байесовского апдейта наблюдения с маленьким количеством наблюдений льнут к среднему априорного распределения. Мы можем даже умножить параметры нашего априорного распределения на 10, чтобы показать это: mikhalevskaja %&gt;% filter(feature == &quot;a-e&quot;) %&gt;% mutate(alpha_prior = beta_est$estimate[1]*10, beta_prior = beta_est$estimate[2]*10, alpha_post = conservative+alpha_prior, beta_post = innovative+beta_prior, mean_post = alpha_post/(alpha_post+beta_post), ratio = conservative/total) %&gt;% ggplot(aes(ratio, mean_post, label = speaker, color = total))+ geom_hline(yintercept = beta_est$estimate[1]/sum(beta_est$estimate), linetype = 2)+ geom_point()+ ggrepel::geom_text_repel() mikhalevskaja %&gt;% filter(feature == &quot;a-e&quot;) %&gt;% mutate(alpha_prior = beta_est$estimate[1]*40, beta_prior = beta_est$estimate[2]*40, alpha_post = conservative+alpha_prior, beta_post = innovative+beta_prior, mean_post = alpha_post/(alpha_post+beta_post), ratio = conservative/total) %&gt;% ggplot(aes(ratio, mean_post, label = speaker, color = total))+ geom_hline(yintercept = beta_est$estimate[1]/sum(beta_est$estimate), linetype = 2)+ geom_point()+ ggrepel::geom_text_repel() Или в формате гифки: В работе (Coretta 2016) собраны данные длительности исландских гласных. Используя алгоритм максимального правдоподобия и идеи эмперической байесовской оценки, найдите априорное распределение для длительности гласных (переменная vowel.dur), используя все наблюдения в датасете и моделируя его нормальным распределением. Дальше проведите байесовский апдейт длительности гласных носителя tt01 (переменная speaker) и нарисуйте 80% доверительный интервал апостериорного распределения. Ссылки на литературу "],["введение-в-марковские-цепи-монте-карло.html", "9 Введение в Марковские цепи Монте-Карло 9.1 Марковские цепи 9.2 Хакерская статистика (основано на “Статистика для хакеров” Джейка Вандерпласа) 9.3 Соединение идей Марковских цепей и Монте-Карло 9.4 brms", " 9 Введение в Марковские цепи Монте-Карло library(tidyverse) Марковская цепь Монте-Карло (Markov chain Monte Carlo, MCMC) — это класс алгоритмов для семплирования, которые позволяют моделировать некоторое распределение вероятностей. При моделировании используют разные алгоритмы, мы будем смотреть на примере алгоритма Метрополиса-Гастингса (Metropolis-Hastings). Для того, чтобы в этом разобраться нам потребуется обсудить: метод Монте-Карло; марковские цепи; алгоритма Метрополиса-Гастингса. 9.1 Марковские цепи Марковский процесс конечное количество состояний вероятность переходов из одного состояния в другое Возьмем наш датасет с sms и посмотрим частоты разных частей речи:: Теперь давайте посмотрим на частотность переходов из одних состояний в другие: Иногда это визуализируют при помощи графов, но в нашем случае (это граф ham) это достаточно бесполезно (наводите на вершины стрелочки, чтобы что-то разглядеть): Можно посмотреть на славную визуализацию (спасибо за ссылку Марине Дубовой). 9.2 Хакерская статистика (основано на “Статистика для хакеров” Джейка Вандерпласа) Вообще симуляции позволяют делать статистику более осмысленной. 9.2.1 Биномиальные данные Немного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%). Можно ли сказать, что перед Вами носитель британского английского, если Вы наблюдаете в интервью актера из 120 контекстов 100 s-генитивов? Вероятность получить 100 успехов из 120 случаев, если мы верим, что вероятность успеха равна 0.9 описывается биномиальным распределением: \\[P(H = h|p, n) = \\binom{n}{h}\\times p^h\\times(1-p)^{1-h}\\] Фреквентистский подход: биномиальный тест H\\(_0\\) человек говорит s-генитив с вероятностью 0.9 α = 0.05 Байесовский подход: биномиальная функция правдоподобия перемножается с априорным бета распределением чтобы получить апостериорное распределение. tibble(h = seq(0, 1, 0.01), y = dbeta(h, shape1 = 100 + 90, shape2 = 20 + 10)) %&gt;% ggplot(aes(h, y))+ geom_line()+ geom_area(aes(x = ifelse(h&gt;=qbeta(0.025, shape1 = 100 + 90, shape2 = 20 + 10) &amp; h&lt;=qbeta(0.975, shape1 = 100 + 90, shape2 = 20 + 10), h, NA)), fill = &quot;darkgreen&quot;)+ geom_vline(xintercept = 100/120, linetype = 2) Хакерский подход: симуляция: set.seed(42) map_dbl(1:1000, function(i){ sample(0:1, 120, replace = TRUE, prob = c(0.1, 0.9)) %&gt;% sum() }) -&gt; simulations tibble(sum = simulations) %&gt;% mutate(greater = sum &lt;= 100) %&gt;% group_by(greater) %&gt;% summarise(number = n()) tibble(sum = simulations) %&gt;% ggplot(aes(sum))+ geom_density()+ geom_vline(xintercept = 100, linetype = 2)+ scale_x_continuous(breaks = c(0:9*20), limits = c(0, 120)) Аналогично можно использовать: случайное перемешивание вместо двухвыборочного t-теста; бутстрэп вместо одновыборочного t-теста. 9.2.2 Метод Монте-Карло Группа методов изучения случайных процессов, которые базируются на: возможности производить бесконечного количества случайных значений для известных или новых распределений Представим себе, что у нас есть какой-то интеграл, который мы хотим посчитать. Например такой: tibble(x = seq(0, 1, length.out = 1000)) %&gt;% ggplot(aes(x))+ stat_function(fun = function(x){x^12-sin(x)+1}, geom = &quot;area&quot;, fill = &quot;lightblue&quot;) Мы можем насэмплировать точек из комбинации двух унимодальных распределений u(0, 1), и u(0, 1.2) и посмотреть, кто попадает в область, а кто нет: set.seed(42) tibble(x = runif(1e3, 0, 1), y = runif(1e3, 0, 1.2), in_area = y &lt; x^12-sin(x)+1) %&gt;% ggplot(aes(x, y))+ stat_function(fun = function(x){x^12-sin(x)+1}, geom = &quot;area&quot;, fill = &quot;lightblue&quot;)+ geom_point(aes(color = in_area), show.legend = FALSE) Сколько попало? set.seed(42) tibble(x = runif(1e3, 0, 1), y = runif(1e3, 0, 1.2), in_area = y &lt; x^12-sin(x)+1) %&gt;% count(in_area) 528/1000*1.2 [1] 0.6336 integrate(function(x){x^12-sin(x)+1}, 0, 1) 0.6172254 with absolute error &lt; 0.0000000000000069 А если увеличить количество наблюдений? set.seed(42) tibble(x = runif(1e7, 0, 1), y = runif(1e7, 0, 1.2), in_area = y &lt; x^12-sin(x)+1) %&gt;% count(in_area) 5143667/10000000*1.2 [1] 0.61724 integrate(function(x){x^12-sin(x)+1}, 0, 1) 0.6172254 with absolute error &lt; 0.0000000000000069 9.3 Соединение идей Марковских цепей и Монте-Карло shiny::runGitHub(&quot;agricolamz/mcmc_shiny&quot;) Основные проблемы MCMC: Зависимость от начального значения. Решение: выкинуть начальную часть цепи (burn-in). Полученные значения автокоррелируют, так как они были получины при помощи марковского процесса. Решение: брать, например, каждое третье значение. Вы можете почитать историю идей MCMC в работе (Robert and Casella 2011) (доступна здесь). 9.4 brms Для вычисления всяких сложных статистических моделей люди придумали вероятностные языки программирования. Чаще всего они являются расширением для стандартных языков программирования, но иногда становятся самодостаточными языками (однако они все равно написаны на каких-то быстрых языках программирования типа C++). Примерами таких самодостаточных языков является: BUGS (Bayesian inference Using Gibbs Sampling) JAGS (Just Another Gibbs Sampler) NIMBLE NUTS (No-U-Turn-Sampler) Stan Для них пишут обертки на разных языках, мы будем использовать пакет brms, который является оберткой над пакетом rstan, который является оберткой для Stan. Вы можете установить эти пакеты к себе на компьютер, но есть высокая вероятность, что что-то пойдет не так, в связи с чем, я предлагаю всем использовать rstudio.cloud для примеров на brms. Чтобы повторить примеры Вам нужно: Установить на свой компьютер rstan (инструкции) и brms Если на вашем компьютере не выходит, попробуйте &lt;rstudio.cloud&gt; 9.4.1 Регрессионный пример В работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, произнесенные носителем tt01 (переменная speaker), и смоделируйте длительность гласных (переменная vowel.dur) нормальным распределением используя в качестве априорного распределения нормальное распределение со средним 275 и стандартным отклонением 65 (основано на (Hillenbrand et al. 1995)). read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/Coretta_2017_icelandic.csv&quot;) %&gt;% filter(speaker == &quot;tt01&quot;) -&gt; vowel_data В качестве первого этапа стоит сформулировать модель и посмотреть, в какой форме от нас ожидают априорное распределение. library(brms) get_prior(vowel.dur ~ 0 + Intercept, family = &quot;normal&quot;, data = vowel_data) Сделаем нашу первую модель: normal_fit &lt;- brm(vowel.dur ~ 0 + Intercept, family = &quot;normal&quot;, data = vowel_data, prior = c(prior(normal(275, 65), coef = Intercept)), silent = TRUE) SAMPLING FOR MODEL &#39;f8620c31edb9ae4be8203def3fec2653&#39; NOW (CHAIN 1). Chain 1: Chain 1: Gradient evaluation took 1e-05 seconds Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. Chain 1: Adjust your expectations accordingly! Chain 1: Chain 1: Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) Chain 1: Chain 1: Elapsed Time: 0.01396 seconds (Warm-up) Chain 1: 0.009019 seconds (Sampling) Chain 1: 0.022979 seconds (Total) Chain 1: SAMPLING FOR MODEL &#39;f8620c31edb9ae4be8203def3fec2653&#39; NOW (CHAIN 2). Chain 2: Chain 2: Gradient evaluation took 5e-06 seconds Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. Chain 2: Adjust your expectations accordingly! Chain 2: Chain 2: Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) Chain 2: Chain 2: Elapsed Time: 0.013054 seconds (Warm-up) Chain 2: 0.009854 seconds (Sampling) Chain 2: 0.022908 seconds (Total) Chain 2: SAMPLING FOR MODEL &#39;f8620c31edb9ae4be8203def3fec2653&#39; NOW (CHAIN 3). Chain 3: Chain 3: Gradient evaluation took 4e-06 seconds Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. Chain 3: Adjust your expectations accordingly! Chain 3: Chain 3: Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) Chain 3: Chain 3: Elapsed Time: 0.012447 seconds (Warm-up) Chain 3: 0.010284 seconds (Sampling) Chain 3: 0.022731 seconds (Total) Chain 3: SAMPLING FOR MODEL &#39;f8620c31edb9ae4be8203def3fec2653&#39; NOW (CHAIN 4). Chain 4: Chain 4: Gradient evaluation took 4e-06 seconds Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. Chain 4: Adjust your expectations accordingly! Chain 4: Chain 4: Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) Chain 4: Chain 4: Elapsed Time: 0.011714 seconds (Warm-up) Chain 4: 0.009651 seconds (Sampling) Chain 4: 0.021365 seconds (Total) Chain 4: normal_fit Family: gaussian Links: mu = identity; sigma = identity Formula: vowel.dur ~ 0 + Intercept Data: vowel_data (Number of observations: 175) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 89.53 1.93 85.76 93.26 1.00 3155 2162 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 24.98 1.28 22.62 27.60 1.00 4167 3039 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). Визуализируем нашу модель: plot(normal_fit) После того как мы сгенерировали наши распределения, мы можем их достать и что-нибудь посчитать: as_draws_array(normal_fit) %&gt;% posterior::summarize_draws() as_draws_rvars(normal_fit) # A draws_rvars: 1000 iterations, 4 chains, and 3 variables $b_Intercept: rvar&lt;1000,4&gt;[1] mean ± sd: [1] 90 ± 1.9 $sigma: rvar&lt;1000,4&gt;[1] mean ± sd: [1] 25 ± 1.3 $lp__: rvar&lt;1000,4&gt;[1] mean ± sd: [1] -822 ± 0.99 Можно визуально оценить, где лежат наши данные, а что предсказывает полученная модель: pp_check(normal_fit) Ссылки на литературу "],["ограничения-на-применение-регрессии.html", "10 Ограничения на применение регрессии 10.1 Дисперсия и стандартное отклонение 10.2 Ковариация 10.3 Корреляция 10.4 Основы регрессионного анализа 10.5 Нелинейность взаимосвязи 10.6 Нормальность распределение остатков 10.7 Гетероскидастичность 10.8 Мультиколлинеарность 10.9 Независимость наблюдений", " 10 Ограничения на применение регрессии library(tidyverse) 10.1 Дисперсия и стандартное отклонение Дисперсия — мера разброса значений наблюдений относительно среднего. \\[\\sigma^2_X = \\frac{\\sum_{i = 1}^n(x_i - \\bar{x})^2}{n - 1},\\] где \\(x_1, ..., x_n\\) — наблюдения; \\(\\bar{x}\\) — среднее всех наблюдений; \\(X\\) — вектор всех наблюдений; \\(n\\) — количество наблюдений. Представим, что у нас есть следующие данные: Тогда дисперсия — это сумма квадратов расстояний от каждой точки до среднего выборки (пунктирная линия) разделенное на количество наблюдений - 1 (по духу эта мера — обычное среднее, но если вас инетересует разница смещенной и несмещенной оценки дисперсии, см. видео). Для того чтобы было понятнее, что такое дисперсия, давайте рассмотрим несколько расспределений с одним и тем же средним, но разными дисперсиями: В R дисперсию можно посчитать при помощи функции var()2. set.seed(42) x &lt;- rnorm(20, mean = 50, sd = 10) var(x) [1] 172.2993 Проверим, что функция выдает то же, что мы записали в формуле. var(x) == sum((x - mean(x))^2)/(length(x)-1) [1] TRUE Так как дисперсия является квадратом отклонения, то часто вместо нее используют более интерпретируемое стандартное отклонение \\(\\sigma\\) — корень из дисперсии. В R ее можно посчитать при помощи функции sd(): sd(x) [1] 13.12628 sd(x) == sqrt(var(x)) [1] TRUE 10.2 Ковариация Ковариация — эта мера ассоциации двух переменных. \\[cov(X, Y) = \\frac{\\sum_{i = 1}^n(x_i - \\bar{x})(y_i-\\bar{y})}{n - 1},\\] где \\((x_1, y_1), ..., (x_n, y_n)\\) — пары наблюдений; \\(\\bar{x}, \\bar{y}\\) — средние наблюдений; \\(X, Y\\) — векторы всех наблюдений; \\(n\\) — количество наблюдений. Представим, что у нас есть следующие данные: Тогда, согласно формуле, для каждой точки вычисляется следующая площадь (пуктирными линиями обозначены средние): Если значения \\(x_i\\) и \\(y_i\\) какой-то точки либо оба больше, либо оба меньше средних \\(\\bar{x}\\) и \\(\\bar{y}\\), то получившееся произведение будет иметь знак +, если же наоборот — знак -. На графике это показано цветом. Таким образом, если много красных прямоугольников, то значение суммы будет положительное и обозначать положительную связь (чем больше \\(x\\), тем больше \\(y\\)), а если будет много синий прямоугольников, то значение суммы отрицательное и обозначать положительную связь (чем больше \\(x\\), тем меньше \\(y\\)). Непосредственно значение ковариации не очень информативно, так как может достаточно сильно варьироваться от датасета к датасету. В R ковариацию можно посчитать при помощи функции cov(). set.seed(42) x &lt;- rnorm(10, mean = 50, sd = 10) y &lt;- x + rnorm(10, sd = 10) cov(x, y) [1] 18.72204 cov(x, -y*2) [1] -37.44407 Как видно, простое умножение на два удвоило значение ковариации, что показывает, что непосредственно ковариацию использовать для сравнения разных датасетов не стоит. Проверим, что функция выдает то же, что мы записали в формуле. cov(x, y) == sum((x-mean(x))*(y - mean(y)))/(length(x)-1) [1] TRUE 10.3 Корреляция Корреляция — это мера ассоциации/связи двух числовых переменных. Помните, что бытовое применение этого термина к категориальным переменным (например, корреляция цвета глаз и успеваемость на занятиях по R) не имеет смысла с точки зрения статистики. 10.3.1 Корреляция Пирсона Коэффициент корреляции Пирсона — базовый коэффициент ассоциации переменных, однако стоит помнить, что он дает неправильную оценку, если связь между переменными нелинейна. \\[\\rho_{X,Y} = \\frac{cov(X, Y)}{\\sigma_X\\times\\sigma_Y} = \\frac{1}{n-1}\\times\\sum_{i = 1}^n\\left(\\frac{x_i-\\bar{x}}{\\sigma_X}\\times\\frac{y_i-\\bar{y}}{\\sigma_Y}\\right),\\] где \\((x_1, y_1), ..., (x_n, y_n)\\) — пары наблюдений; \\(\\bar{x}, \\bar{y}\\) — средние наблюдений; \\(X, Y\\) — векторы всех наблюдений; \\(n\\) — количество наблюдений. Последнее уравнение показывает, что коэффициент корреляции Пирсона можно представить как среднее (с поправкой, поэтому \\(n-1\\), а не \\(n\\)) произведение \\(z\\)-нормализованных значений двух переменных. Эта нормализация приводит к тому, что значения корреляции имеют те же свойства знака коэффициента что и ковариация: если коэффициент положительный (т. е. много красных прямоугольников) — связь между переменными положительная (чем больше \\(x\\), тем больше \\(y\\)), если коэффициент отрицательный (т. е. много синих прямоугольников) — связь между переменными отрицательная (чем больше \\(x\\), тем меньше \\(y\\)); значение корреляции имееет независимое от типа данных интеретация: если модуль коэффициента близок к 1 или ему равен — связь между переменными сильная, если модуль коэффициента близок к 0 или ему равен — связь между переменными слабая. Для того чтобы было понятнее, что такое корреляция, давайте рассмотрим несколько расспределений с разными значениями корреляции: Как видно из этого графика, чем ближе модуль корреляции к 1, тем боллее компактно расположены точки друг к другу, чем ближе к 0, тем более рассеяны значения. Достаточно легко научиться приблизительно оценивать коэфициент корреляции на глаз, поиграв 2–5 минут в игру “Угадай корреляцию” здесь или здесь. В R коэффициент корреляции Пирсона можно посчитать при помощи функции cor(). set.seed(42) x &lt;- rnorm(15, mean = 50, sd = 10) y &lt;- x + rnorm(15, sd = 10) cor(x, y) [1] 0.6659041 Проверим, что функция выдает то же, что мы записали в формуле. cor(x, y) == cov(x, y)/(sd(x)*sd(y)) [1] TRUE cor(x, y) == sum(scale(x)*scale(y))/(length(x)-1) [1] TRUE 10.4 Основы регрессионного анализа Когда мы пытаемся научиться предсказывать данные одной переменной \\(Y\\) при помощи другой переменной \\(X\\), мы получаем формулу: \\[y_i = \\hat\\beta_0 + \\hat\\beta_1 \\times x_i + \\epsilon_i,\\] где \\(x_i\\) — \\(i\\)-ый элемент вектора значений \\(X\\); \\(y_i\\) — \\(i\\)-ый элемент вектора значений \\(Y\\); \\(\\hat\\beta_0\\) — оценка случайного члена (intercept); \\(\\hat\\beta_1\\) — оценка углового коэффициента (slope); \\(\\epsilon_i\\) — \\(i\\)-ый остаток, разница между оценкой модели (\\(\\hat\\beta_0 + \\hat\\beta_1 \\times x_i\\)) и реальным значением \\(y_i\\); весь вектор остатков иногда называют случайным шумом (на графике выделены красным). Причем, иногда мы можем один или другой параметр считать равным нулю. Определите по графику формулу синей прямой. Задача регрессии — оценить параметры \\(\\hat\\beta_0\\) и \\(\\hat\\beta_1\\), если нам известны все значения \\(x_i\\) и \\(y_i\\) и мы пытаемся минимизировать значния \\(\\epsilon_i\\). В данном конкретном случае, задачу можно решить аналитически и получить следующие формулы: \\[\\hat\\beta_1 = \\frac{(\\sum_{i=1}^n x_i\\times y_i)-n\\times\\bar x \\times \\bar y}{\\sum_{i = 1}^n(x_i-\\bar x)^2}\\] \\[\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\times\\bar x\\] При этом, вне зависимости от статистической школы, у регрессии есть свои ограничения на применение: линейность связи между \\(x\\) и \\(y\\); нормальность распределение остатков \\(\\epsilon_i\\); гомоскидастичность — равномерность распределения остатков на всем протяжении \\(x\\); независимость переменных; независимость наблюдений друг от друга. 10.4.1 Первая регрессия Давайте попробуем смоделировать количество слов и в рассказах М. Зощенко в зависимости от длины рассказа: zo &lt;- read_tsv(&quot;https://github.com/agricolamz/DS_for_DH/raw/master/data/tidy_zoshenko.csv&quot;) zo %&gt;% filter(word == &quot;и&quot;) %&gt;% distinct() %&gt;% ggplot(aes(n_words, n))+ geom_point()+ labs(x = &quot;количество слов в рассказе&quot;, y = &quot;количество и&quot;) Мы видим, несколько одиночных точек, давайте избавимся от них и добавим регрессионную линию при помощи функции geom_smooth(): zo %&gt;% filter(word == &quot;и&quot;, n_words &lt; 1500) %&gt;% distinct() -&gt; zo_filtered zo_filtered %&gt;% ggplot(aes(n_words, n))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, se = FALSE)+ labs(x = &quot;количество слов в рассказе&quot;, y = &quot;количество и&quot;) Чтобы получить формулу этой линии нужно запустить функцию, которая оценивает линейную регрессию: fit &lt;- lm(n~n_words, data = zo_filtered) fit Call: lm(formula = n ~ n_words, data = zo_filtered) Coefficients: (Intercept) n_words -1.47184 0.04405 Вот мы и получили коэффициенты, теперь мы видим, что наша модель считает следующее: \\[n = -1.47184 + 0.04405 \\times n\\_words\\] Более подробную информцию можно посмотреть, если запустить модель в функцию summary(): summary(fit) Call: lm(formula = n ~ n_words, data = zo_filtered) Residuals: Min 1Q Median 3Q Max -19.6830 -4.3835 0.8986 4.6486 19.6413 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.471840 2.467149 -0.597 0.553 n_words 0.044049 0.003666 12.015 &lt;0.0000000000000002 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 7.945 on 64 degrees of freedom Multiple R-squared: 0.6928, Adjusted R-squared: 0.688 F-statistic: 144.4 on 1 and 64 DF, p-value: &lt; 0.00000000000000022 В разделе Coefficients содержится информацию про наши коэффициенты: Estimate – полученная оценка коэффициентов; Std. Error – стандартная ошибка среднего; t value – \\(t\\)-статистика, полученная при проведении одновыборочного \\(t\\)-теста, сравнивающего данный коэфициент с 0; Pr(&gt;|t|) – полученное \\(p\\)-значение; Multiple R-squared и Adjusted R-squared — одна из оценок модели, показывает связь между переменными. Без поправок совпадает с квадратом коэффициента корреляции Пирсона: cor(zo_filtered$n_words, zo_filtered$n)^2 [1] 0.6928376 F-statistic — \\(F\\)-статистика полученная при проведении теста, проверяющего, не являются ли хотя бы один из коэффицинтов статистически значимо отличается от нуля. Совпадает с результатами дисперсионного анализа (ANOVA). Теперь мы можем даже предсказывать значения, которые мы еще не видели. Например, сколько будет и в рассказе Зощенко длиной 1000 слов? predict(fit, tibble(n_words = 1000)) 1 42.57715 10.4.2 Категориальные переменные Что если мы хотим включить в наш анализ категориальные переменные? Давайте рассмотрим простой пример с рассказами Чехова и Зощенко, которые мы рассматривали в прошлом разделе. Мы будем анализировать логарифм доли слов деньги: chekhov &lt;- read_tsv(&quot;https://github.com/agricolamz/DS_for_DH/raw/master/data/tidy_chekhov.tsv&quot;) zoshenko &lt;- read_tsv(&quot;https://github.com/agricolamz/DS_for_DH/raw/master/data/tidy_zoshenko.csv&quot;) chekhov$author &lt;- &quot;Чехов&quot; zoshenko$author &lt;- &quot;Зощенко&quot; chekhov %&gt;% bind_rows(zoshenko) %&gt;% filter(str_detect(word, &quot;деньг&quot;)) %&gt;% group_by(author, titles, n_words) %&gt;% summarise(n = sum(n)) %&gt;% mutate(log_ratio = log(n/n_words)) -&gt; checkov_zoshenko Визуализация выглядит так: Красной точкой обозначены средние значения, так что мы видим, что между двумя писателями есть разница, но является ли она статистически значимой? В прошлом разделе, мы рассмотрели, что в таком случае можно сделать t-test: t.test(log_ratio~author, data = checkov_zoshenko, var.equal =TRUE) # здесь я мухлюю, отключая поправку Уэлча Two Sample t-test data: log_ratio by author t = 5.6871, df = 125, p-value = 0.00000008665 alternative hypothesis: true difference in means between group Зощенко and group Чехов is not equal to 0 95 percent confidence interval: 0.8606107 1.7793181 sample estimates: mean in group Зощенко mean in group Чехов -5.021262 -6.341226 Разница между группами является статистически значимой (t(125) = 5.6871, p-value = 8.665e-08). Для того, чтобы запустить регрессию на категориальных данных категориальная переменная автоматически разбивается на группу бинарных dummy-переменных: tibble(author = c(&quot;Чехов&quot;, &quot;Зощенко&quot;), dummy_chekhov = c(1, 0), dummy_zoshenko = c(0, 1)) Дальше для регрессионного анализа выкидывают одну из переменных, так как иначе модель не сойдется (dummy-переменных всегда n-1, где n — количество категорий в переменной). tibble(author = c(&quot;Чехов&quot;, &quot;Зощенко&quot;), dummy_chekhov = c(1, 0)) Если переменная dummy_chekhov принимает значение 1, значит речь о рассказе Чехова, а если принимает значение 0, то о рассказе Зощенко. Если вставить нашу переменную в регрессионную формулу получится следующее: \\[y_i = \\hat\\beta_0 + \\hat\\beta_1 \\times \\text{dummy_chekhov} + \\epsilon_i,\\] Так как dummy_chekhov принимает либо значение 1, либо значение 0, то получается, что модель предсказывает лишь два значения: \\[y_i = \\left\\{\\begin{array}{ll}\\hat\\beta_0 + \\hat\\beta_1 \\times 1 + \\epsilon_i = \\hat\\beta_0 + \\hat\\beta_1 + \\epsilon_i\\text{, если рассказ Чехова}\\\\ \\hat\\beta_0 + \\hat\\beta_1 \\times 0 + \\epsilon_i = \\hat\\beta_0 + \\epsilon_i\\text{, если рассказ Зощенко} \\end{array}\\right.\\] Таким образом, получается, что свободный член \\(\\beta_0\\) и угловой коэффициент \\(\\beta_1\\) в регресси с категориальной переменной получает другую интерпретацию. Одно из значений переменной кодируется при помощи \\(\\beta_0\\), а сумма коэффициентов \\(\\beta_0+\\beta_1\\) дают другое значение переменной. Так что \\(\\beta_1\\) — это разница между оценками двух значений переменной. Давайте теперь запустим регрессию на этих же данных: fit2 &lt;- lm(log_ratio~author, data = checkov_zoshenko) summary(fit2) Call: lm(formula = log_ratio ~ author, data = checkov_zoshenko) Residuals: Min 1Q Median 3Q Max -2.8652 -0.6105 -0.0607 0.6546 3.2398 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -5.0213 0.2120 -23.680 &lt; 0.0000000000000002 *** authorЧехов -1.3200 0.2321 -5.687 0.0000000867 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.9717 on 125 degrees of freedom Multiple R-squared: 0.2056, Adjusted R-squared: 0.1992 F-statistic: 32.34 on 1 and 125 DF, p-value: 0.00000008665 Во-первых стоит обратить внимание на то, что R сам преобразовал нашу категориальную переменную в dummy-переменную authorЧехов. Во-вторых, можно заметить, что значения t-статистики и p-value совпадают с результатами полученными нами в t-тесте выше. Статистическти значимый коэффициент при аргументе authorЧехов следует интерпретировать как разницу средних между логарифмом долей в рассказах Чехова и Зощенко. 10.4.3 Множественная регрессия Множественная регрессия позволяет проанализировать связь между зависимой и несколькими зависимыми переменными. Формула множественной регрессии не сильно отличается от формулы обычной линейной регрессии: \\[y_i = \\hat\\beta_0 + \\hat\\beta_1 \\times x_{1i}+ \\dots+ \\hat\\beta_n \\times x_{ni} + \\epsilon_i,\\] \\(x_{ki}\\) — \\(i\\)-ый элемент векторов значений \\(X_1, \\dots, X_n\\); \\(y_i\\) — \\(i\\)-ый элемент вектора значений \\(Y\\); \\(\\hat\\beta_0\\) — оценка случайного члена (intercept); \\(\\hat\\beta_k\\) — коэфциент при переменной \\(X_{k}\\); \\(\\epsilon_i\\) — \\(i\\)-ый остаток, разница между оценкой модели (\\(\\hat\\beta_0 + \\hat\\beta_1 \\times x_i\\)) и реальным значением \\(y_i\\); весь вектор остатков иногда называют случайным шумом. В такой регресии предикторы могут быть как числовыми, так и категориальными (со всеми вытекающими последствиями, которые мы обсудили в предудщем разделе). Такую регрессию чаще всего сложно визуализировать, так как в одну регрессионную линию вкладываются сразу несколько переменных. Попробуем предсказать длину лепестка на основе длины чашелистик и вида ириса: iris %&gt;% ggplot(aes(Sepal.Length, Petal.Length, color = Species))+ geom_point() Запустим регрессию: fit3 &lt;- lm(Petal.Length ~ Sepal.Length+ Species, data = iris) summary(fit3) Call: lm(formula = Petal.Length ~ Sepal.Length + Species, data = iris) Residuals: Min 1Q Median 3Q Max -0.76390 -0.17875 0.00716 0.17461 0.79954 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.70234 0.23013 -7.397 0.0000000000101 *** Sepal.Length 0.63211 0.04527 13.962 &lt; 0.0000000000000002 *** Speciesversicolor 2.21014 0.07047 31.362 &lt; 0.0000000000000002 *** Speciesvirginica 3.09000 0.09123 33.870 &lt; 0.0000000000000002 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2826 on 146 degrees of freedom Multiple R-squared: 0.9749, Adjusted R-squared: 0.9744 F-statistic: 1890 on 3 and 146 DF, p-value: &lt; 0.00000000000000022 Все предикторы статистически значимы. Давайте посмотрим предсказания модели для всех наблюдений: iris %&gt;% mutate(prediction = predict(fit3)) %&gt;% ggplot(aes(Sepal.Length, prediction, color = Species))+ geom_point() Всегда имеет смысл визуализировать, что нам говорит наша модель. Если использовать пакет ggeffects (или предшествовавший ему пакет effects), это можно сделать не сильно задумываясь, как это делать: library(ggeffects) plot(ggpredict(fit3, terms = c(&quot;Sepal.Length&quot;, &quot;Species&quot;))) Как видно из графиков, наша модель имеет одинаковые угловые коэффициенты (slope) для каждого из видов ириса и разные свободные члены (intercept). summary(fit3) Call: lm(formula = Petal.Length ~ Sepal.Length + Species, data = iris) Residuals: Min 1Q Median 3Q Max -0.76390 -0.17875 0.00716 0.17461 0.79954 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.70234 0.23013 -7.397 0.0000000000101 *** Sepal.Length 0.63211 0.04527 13.962 &lt; 0.0000000000000002 *** Speciesversicolor 2.21014 0.07047 31.362 &lt; 0.0000000000000002 *** Speciesvirginica 3.09000 0.09123 33.870 &lt; 0.0000000000000002 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2826 on 146 degrees of freedom Multiple R-squared: 0.9749, Adjusted R-squared: 0.9744 F-statistic: 1890 on 3 and 146 DF, p-value: &lt; 0.00000000000000022 \\[y_i = \\left\\{\\begin{array}{ll} -1.70234 + 0.63211 \\times \\text{Sepal.Length} + \\epsilon_i\\text{, если вид setosa}\\\\ -1.70234 + 2.2101 + 0.63211 \\times \\text{Sepal.Length} + \\epsilon_i\\text{, если вид versicolor} \\\\ -1.70234 + 3.09 + 0.63211 \\times \\text{Sepal.Length} + \\epsilon_i\\text{, если вид virginica} \\end{array}\\right.\\] 10.5 Нелинейность взаимосвязи Давайте восползуемся данными из пакета Rling Натальи Левшиной. В датасете 100 произвольно выбранных слов из проекта English Lexicon Project (Balota et al. 2007), их длина, среднее время реакции и частота в корпусе. ldt &lt;- read_csv(&quot;https://goo.gl/ToxfU6&quot;) ldt Давайте посмотрим на простой график: ldt %&gt;% ggplot(aes(Mean_RT, Freq))+ geom_point()+ theme_bw() Регрессия на таких данных будет супер неиформативна: ldt %&gt;% ggplot(aes(Mean_RT, Freq))+ geom_point()+ geom_smooth(method = &quot;lm&quot;)+ theme_bw() m1 &lt;- summary(lm(Mean_RT~Freq, data = ldt)) m1 Call: lm(formula = Mean_RT ~ Freq, data = ldt) Residuals: Min 1Q Median 3Q Max -224.93 -85.42 -30.52 81.90 632.66 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 826.998242 15.229783 54.301 &lt; 0.0000000000000002 *** Freq -0.005595 0.001486 -3.765 0.000284 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 143.9 on 98 degrees of freedom Multiple R-squared: 0.1264, Adjusted R-squared: 0.1174 F-statistic: 14.17 on 1 and 98 DF, p-value: 0.0002843 10.5.1 Логарифмирование ldt %&gt;% ggplot(aes(Mean_RT, log(Freq)))+ geom_point()+ geom_smooth(method = &quot;lm&quot;)+ theme_bw() ldt %&gt;% ggplot(aes(Mean_RT, log(Freq+1)))+ geom_point()+ geom_smooth(method = &quot;lm&quot;)+ theme_bw() m2 &lt;- summary(lm(Mean_RT~log(Freq+1), data = ldt)) m2 Call: lm(formula = Mean_RT ~ log(Freq + 1), data = ldt) Residuals: Min 1Q Median 3Q Max -242.36 -76.66 -17.49 48.64 630.49 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1001.60 29.79 33.627 &lt; 0.0000000000000002 *** log(Freq + 1) -34.03 4.76 -7.149 0.000000000158 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 124.8 on 98 degrees of freedom Multiple R-squared: 0.3428, Adjusted R-squared: 0.3361 F-statistic: 51.11 on 1 and 98 DF, p-value: 0.0000000001576 m1$adj.r.squared [1] 0.1174405 m2$adj.r.squared [1] 0.336078 Отлогорифмировать можно и другую переменную. ldt %&gt;% ggplot(aes(log(Mean_RT), log(Freq + 1)))+ geom_point()+ geom_smooth(method = &quot;lm&quot;)+ theme_bw() m3 &lt;- summary(lm(log(Mean_RT)~log(Freq+1), data = ldt)) m1$adj.r.squared [1] 0.1174405 m2$adj.r.squared [1] 0.336078 m3$adj.r.squared [1] 0.3838649 Как интерпретировать полученную регрессию с двумя отлогорифмированными значениями? В обычной линейной регресии мы узнаем отношения между \\(x\\) и \\(y\\): \\[y_i = \\beta_0+\\beta_1\\times x_i\\] Как изменится \\(y_j\\), если мы увеличем \\(x_i + 1 = x_j\\)? \\[y_j = \\beta_0+\\beta_1\\times x_j\\] \\[y_j - y_i = \\beta_0+\\beta_1\\times x_j - (\\beta_0+\\beta_1\\times x_i) = \\beta_1(x_j - x_i)\\] Т. е. \\(y\\) увеличится на \\(\\beta_1\\) , если \\(x\\) увеличится на 1. Что же будет с логарифмированными переменными? Как изменится \\(y_j\\), если мы увеличем \\(x_i + 1 = x_j\\)? \\[\\log(y_j) - \\log(y_i) = \\beta_1\\times (\\log(x_j) - \\log(x_i))\\] \\[\\log\\left(\\frac{y_j}{y_i}\\right) = \\beta_1\\times \\log\\left(\\frac{x_j}{x_i}\\right) = \\log\\left(\\left(\\frac{x_j}{x_i}\\right) ^ {\\beta_1}\\right)\\] \\[\\frac{y_j}{y_i}= \\left(\\frac{x_j}{x_i}\\right) ^ {\\beta_1}\\] Т. е. \\(y\\) увеличится на \\(\\beta_1\\) процентов, если \\(x\\) увеличится на 1 процент. Логарифмирование — не единственный вид траснформации: трансформация Тьюки shiny::runGitHub(&quot;agricolamz/tukey_transform&quot;) трансформация Бокса — Кокса … В датасет собрана частотность разных лемм на основании корпуса НКРЯ (Ляшевская and Шаров 2009) (в датасете только значения больше ipm &gt; 10). Известно, что частотность слова связана с рангом слова (см. закон Ципфа). Постройте переменную ранга и визуализируйте связь ранга и логорифма частотности с разбивкой по частям речи. Какие части речи так и не приобрели после трансформации “приемлимую” линейную форму? (я насчитал 5 таких) aadvadvproanumaproconjintjnumpartprss.PROPsprov 10.6 Нормальность распределение остатков Линейная регрессия предполагает нормальность распределения остатков. Когда связь не линейна, то остатки тоже будут распределены не нормально. Можно смотреть на первый график используя функцию plot(m1) — график остатков. Интерпретаций этого графика достаточно много (см. статью про это). Можно смотреть на qqplot: tibble(res = m1$residuals) %&gt;% ggplot(aes(res))+ geom_histogram(aes(y = ..density..))+ stat_function(fun = dnorm, args = list(mean = 0, sd = sd(m1$residuals)), color = &quot;red&quot;) qqnorm(m1$residuals) qqline(m1$residuals) tibble(res = m2$residuals) %&gt;% ggplot(aes(res))+ geom_histogram(aes(y = ..density..))+ stat_function(fun = dnorm, args = list(mean = 0, sd = sd(m2$residuals)), color = &quot;red&quot;) qqnorm(m2$residuals) qqline(m2$residuals) tibble(res = m3$residuals) %&gt;% ggplot(aes(res))+ geom_histogram(aes(y = ..density..))+ stat_function(fun = dnorm, args = list(mean = 0, sd = sd(m3$residuals)), color = &quot;red&quot;) qqnorm(m3$residuals) qqline(m3$residuals) 10.7 Гетероскидастичность Распределение остатков непостоянно (т.е. не гомоскидастичны): ldt %&gt;% ggplot(aes(Mean_RT, Freq))+ geom_point()+ theme_bw() Тоже решается преобазованием данных. 10.8 Мультиколлинеарность Линейная связь между некоторыми предикторами в модели. корреляционная матрица VIF (Variance inflation factor), car::vif() VIF = 1 (Not correlated) 1 &lt; VIF &lt; 5 (Moderately correlated) VIF &gt;=5 (Highly correlated) 10.9 Независимость наблюдений Наблюдения должны быть независимы. В ином случае нужно использовать модель со смешанными эффектами. 10.9.1 Линейная модель со смешанными эффектами В качестве примера мы попробуем поиграть с законом Хердана-Хипса, описывающий взаимосвязь количества уникальных слов в тексте в зависимости от длины текста. В датасете собраны некоторые корпуса Universal Dependencies (Zeman et al. 2020) и некоторые числа, посчитанные на их основании: ud &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/ud_corpora.csv&quot;) ud %&gt;% ggplot(aes(n_words, n_tokens))+ geom_point()+ facet_wrap(~corpus, scale = &quot;free&quot;)+ geom_smooth(method = &quot;lm&quot;, se = FALSE)+ labs(x = &quot;количество слов&quot;, y = &quot;количество уникальных слов&quot;, caption = &quot;данные корпусов Universal Dependencies&quot;) Связь между переменными безусловно линейная, однако в разных корпусах представлена разная перспектива: для каких-то корпусов, видимо, тексты специально нарезались, так что тексты таких корпусов содержат от 30-40 до 50-80 слов, а какие-то оставались не тронутыми. Чтобы показать, что связь есть, нельзя просто “слить” все наблюдения в один котел (см. парадокс Симпсона), так как это нарушит предположение регрессии о независимости наблюдений. Мы не можем включить переменную corpus в качестве dummy-переменной: тогда один из корпусов попадет в интерсепт (станет своего рода базовым уровенем), а остальные будут от него отсчитываться. К тому же не очень понятно, как работать с новыми данными из других корпусов: ведь мы хотим предсказывать значения обобщенно, вне зависимости от корпуса. При моделировании при помощи моделей со случайными эффектами различают: основные эффекты – это те связи, которые нас интересуют, независимые переменные (количество слов, количество уникальных слов); случайные эффекты – это те переменные, которые создают группировку в данных (корпус). В результате моделирования появляется обобщенная модель, которая игнорирует группировку, а потом для каждого значения случайного эффекта генерируется своя регрессия, отсчитывая от обобщенной модели как от базового уровня. Рассмотрим простейший случай: library(lme4) library(lmerTest) fit1 &lt;- lmer(n_tokens~n_words+(1|corpus), data = ud) summary(fit1) Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ lmerModLmerTest] Formula: n_tokens ~ n_words + (1 | corpus) Data: ud REML criterion at convergence: 10321.5 Scaled residuals: Min 1Q Median 3Q Max -7.5271 -0.4947 0.0354 0.5282 8.6350 Random effects: Groups Name Variance Std.Dev. corpus (Intercept) 240.608 15.512 Residual 8.844 2.974 Number of obs: 2046, groups: corpus, 6 Fixed effects: Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) -4.527634 6.353147 4.958748 -0.713 0.508 n_words 0.827933 0.004418 1992.686553 187.415 &lt;0.0000000000000002 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Correlation of Fixed Effects: (Intr) n_words -0.079 ud %&gt;% mutate(predicted = predict(fit1)) %&gt;% ggplot(aes(n_words, n_tokens))+ geom_point()+ facet_wrap(~corpus, scale = &quot;free&quot;)+ geom_line(aes(y = predicted), color = &quot;red&quot;) + labs(x = &quot;количество слов&quot;, y = &quot;количество уникальных слов&quot;, caption = &quot;данные корпусов Universal Dependencies&quot;) Можно посмотреть на предсказания модели (основные эффекты): library(ggeffects) ggeffect(fit1) %&gt;% plot() $n_words Визуализируйте полученные модели при помощи функции plot(). Какие ограничения на применение линейной регрессии нарушается в наших моделях? plot(fit1) В данном случае мы предполагаем, что случайный эффект имеет случайный свободный член. Т.е. все получающиеся линии параллельны, так как имеют общий угловой коэффициент. Можно допустить большую свободу и сделать так, чтобы в случайном эффекте были не только интерсепт, но и свободный член: fit2 &lt;- lmer(n_tokens~n_words+(1+n_words|corpus), data = ud) summary(fit2) Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ lmerModLmerTest] Formula: n_tokens ~ n_words + (1 + n_words | corpus) Data: ud REML criterion at convergence: 10275.2 Scaled residuals: Min 1Q Median 3Q Max -7.8337 -0.5003 0.0293 0.5172 8.8405 Random effects: Groups Name Variance Std.Dev. Corr corpus (Intercept) 4.465751 2.11323 n_words 0.009532 0.09763 -1.00 Residual 8.693060 2.94840 Number of obs: 2046, groups: corpus, 6 Fixed effects: Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 3.23103 0.88937 2.21775 3.633 0.0582 . n_words 0.80323 0.04005 4.10414 20.056 0.0000299 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Correlation of Fixed Effects: (Intr) n_words -0.988 optimizer (nloptwrap) convergence code: 0 (OK) boundary (singular) fit: see help(&#39;isSingular&#39;) ud %&gt;% mutate(predicted = predict(fit2)) %&gt;% ggplot(aes(n_words, n_tokens))+ geom_point()+ facet_wrap(~corpus, scale = &quot;free&quot;)+ geom_line(aes(y = predicted), color = &quot;red&quot;) + labs(x = &quot;количество слов&quot;, y = &quot;количество уникальных слов&quot;, caption = &quot;данные корпусов Universal Dependencies&quot;) Можно посмотреть на предсказания модели (основные эффекты): ggeffect(fit2) %&gt;% plot() $n_words Нарушения все те же: plot(fit2) При желании мы можем также построить модель, в которой в случайном эффекте будет лишь угловой коэффициент, а свободный член будет фиксированным: fit3 &lt;- lmer(n_tokens~n_words+(0+n_words|corpus), data = ud) summary(fit3) Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ lmerModLmerTest] Formula: n_tokens ~ n_words + (0 + n_words | corpus) Data: ud REML criterion at convergence: 10280.9 Scaled residuals: Min 1Q Median 3Q Max -7.8107 -0.4933 0.0315 0.5227 8.8209 Random effects: Groups Name Variance Std.Dev. corpus n_words 0.004023 0.06343 Residual 8.717996 2.95263 Number of obs: 2046, groups: corpus, 6 Fixed effects: Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 2.64805 0.21627 2043.59791 12.24 &lt; 0.0000000000000002 *** n_words 0.80427 0.02615 5.16230 30.76 0.000000477 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Correlation of Fixed Effects: (Intr) n_words -0.132 ud %&gt;% mutate(predicted = predict(fit3)) %&gt;% ggplot(aes(n_words, n_tokens))+ geom_point()+ facet_wrap(~corpus, scale = &quot;free&quot;)+ geom_line(aes(y = predicted), color = &quot;red&quot;) + labs(x = &quot;количество слов&quot;, y = &quot;количество уникальных слов&quot;, caption = &quot;данные корпусов Universal Dependencies&quot;) Линии получились очень похожими, но разными: Можно посмотреть на предсказания модели (основные эффекты): ggeffect(fit3) %&gt;% plot() $n_words Нарушения все те же: plot(fit3) Сравним полученные модели: anova(fit3, fit2, fit1) Постройте модель со случайными угловым коэффициентом и свободным членом, устранив проблему, которую вы заметили в прошлом задании. Пользуясь знаниями из предыдущих заданий, смоделируйте связь количества слов и количества существительных. С какими проблемами вы столкнулись? Ссылки на литературу "],["байесовский-регрессионный-анализ.html", "11 Байесовский регрессионный анализ 11.1 Основы регрессионного анализа 11.2 brms", " 11 Байесовский регрессионный анализ library(tidyverse) 11.1 Основы регрессионного анализа Когда мы используем регрессионный анализ, мы пытаемся оценить два параметра: свободный член (intercept) – значение \\(y\\) при \\(x = 0\\); угловой коэффициент (slope) – изменение \\(y\\) при изменении \\(x\\) на одну единицу. \\[y_i = \\hat{\\beta_0} + \\hat{\\beta_1}\\times x_i + \\epsilon_i\\] Причем, иногда мы можем один или другой параметр считать равным нулю. При этом, вне зависимости от статистической школы, у регрессии есть свои ограничения на применение: линейность связи между \\(x\\) и \\(y\\); нормальность распределение остатков \\(\\epsilon_i\\); гомоскидастичность — равномерность распределения остатков на всем протяжении \\(x\\); независимость переменных; независимость наблюдений друг от друга. 11.2 brms Для анализа возьмем датасет, который я составил из UD-корпусов и попробуем смоделировать связь между количеством слов в тексте и количеством уникальных слов (закон Хердана-Хипса). ud &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/udpipe_count_n_words_and_tokens/master/filtered_dataset.csv&quot;) glimpse(ud) Rows: 20,705 Columns: 5 $ doc_id &lt;chr&gt; &quot;KR1d0052_001&quot;, &quot;KR1d0052_002&quot;, &quot;KR1d0052_003&quot;, &quot;KR1d0052_… $ n_words &lt;dbl&gt; 3516, 2131, 4927, 4884, 4245, 5027, 3406, 2202, 2673, 2300… $ n_tokens &lt;dbl&gt; 842, 546, 869, 883, 737, 1085, 494, 443, 573, 578, 660, 87… $ language &lt;chr&gt; &quot;Classical_Chinese&quot;, &quot;Classical_Chinese&quot;, &quot;Classical_Chine… $ corpus_code &lt;chr&gt; &quot;Kyoto&quot;, &quot;Kyoto&quot;, &quot;Kyoto&quot;, &quot;Kyoto&quot;, &quot;Kyoto&quot;, &quot;Kyoto&quot;, &quot;Kyo… Для начала, нарушим кучу ограничений на применение регрессии и смоделируем модель для вот таких вот данных, взяв только тексты меньше 1500 слов: ud %&gt;% filter(n_words &lt; 1500) -&gt; ud ud %&gt;% ggplot(aes(n_words, n_tokens))+ geom_point() 11.2.1 Модель только со свободным членом library(brms) parallel::detectCores() [1] 16 n_cores &lt;- 15 # parallel::detectCores() - 1 get_prior(n_tokens ~ 1, data = ud) Вот модель с встроенными априорными распределениями: fit_intercept &lt;- brm(n_tokens ~ 1, data = ud, cores = n_cores, refresh = 0, silent = TRUE) Running /usr/lib/R/bin/R CMD SHLIB foo.c gcc -I&quot;/usr/share/R/include&quot; -DNDEBUG -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/Rcpp/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/unsupported&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/BH/include&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/src/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppParallel/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -fpic -g -O2 -fdebug-prefix-map=/build/r-base-a3XuZ5/r-base-4.2.2.20221110=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -c foo.c -o foo.o In file included from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/Core:88, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/Dense:1, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13, from &lt;command-line&gt;: /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name ‘namespace’ 628 | namespace Eigen { | ^~~~~~~~~ /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:17: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘{’ token 628 | namespace Eigen { | ^ In file included from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/Dense:1, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13, from &lt;command-line&gt;: /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/Core:96:10: fatal error: complex: No such file or directory 96 | #include &lt;complex&gt; | ^~~~~~~~~ compilation terminated. make: *** [/usr/lib/R/etc/Makeconf:169: foo.o] Error 1 При желании встроенные априорные расспеределения можно не использовать и вставлять в аргумент prior априорные распределения по вашему желанию. fit_intercept Family: gaussian Links: mu = identity; sigma = identity Formula: n_tokens ~ 1 Data: ud (Number of observations: 20282) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 135.63 0.86 133.97 137.34 1.00 2730 2345 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 121.76 0.59 120.61 122.97 1.00 3872 2689 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). plot(fit_intercept) 11.2.2 Проверка сходимости модели Вы пострили регрессию, и самый простой способ проверить сходимость это визуально посмотреть на цепи: Running /usr/lib/R/bin/R CMD SHLIB foo.c gcc -I&quot;/usr/share/R/include&quot; -DNDEBUG -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/Rcpp/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/unsupported&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/BH/include&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/src/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppParallel/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -fpic -g -O2 -fdebug-prefix-map=/build/r-base-a3XuZ5/r-base-4.2.2.20221110=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -c foo.c -o foo.o In file included from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/Core:88, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/Dense:1, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13, from &lt;command-line&gt;: /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name ‘namespace’ 628 | namespace Eigen { | ^~~~~~~~~ /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:17: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘{’ token 628 | namespace Eigen { | ^ In file included from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/Dense:1, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13, from &lt;command-line&gt;: /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/Core:96:10: fatal error: complex: No such file or directory 96 | #include &lt;complex&gt; | ^~~~~~~~~ compilation terminated. make: *** [/usr/lib/R/etc/Makeconf:169: foo.o] Error 1 Дальнейший фрагемнт взят из документации Stan (Stan — это один из популярнейших сэмплеров для MCMC и многого другого, который используется в brms). 11.2.2.1 R-hat R-hat – это диагностика сходимости, которая сравнивает оценку модели, которая получается внутри цепи и между цепями (введена в (Gelman and Rubin 1992)). Если цепи не перемешались (например, если нет согласия внутри цепи и между цепями) R-hat будет иметь значение больше 1. Собственно для этого рекоммендуется запускать по-крайней мере 4 цепи. Stan возвращает что-то более сложное, что называется maximum of rank normalized split-R-hat и rank normalized folded-split-R-hat, однако мы не будем в этом разбиратсья, отсылаю к статье или к посту в блоге одного из авторов с объяснением. Самое главное: \\(\\hat{R} \\leq 1.01\\) – значит проблем со сходмиостью цепей не обнаружено. 11.2.2.2 Effective sample size (ESS) Effective sample size (ESS) — это оценка размера выборки достаточной для того, чтобы получить результат такой же точности при помощи случайной выборки из генеральной совокупности. Эту меру используют для оценки размера выборки, в анализе временных рядов и в байесовской статистике. Так как в байесовской статистике мы используем сэмпл из апостериорного распределения для статистического вывода, а значения в цепях коррелируют, то ESS пытается оценить, сколько нужно независимых сэмплов, чтобы получить такую же точность, что и результат MCMC. Чем выше ESS – тем лучше. Stan возвращает две меры: Bulk-ESS Roughly speaking, the effective sample size (ESS) of a quantity of interest captures how many independent draws contain the same amount of information as the dependent sample obtained by the MCMC algorithm. Clearly, the higher the ESS the better. Stan uses R-hat adjustment to use the between-chain information in computing the ESS. For example, in case of multimodal distributions with well-separated modes, this leads to an ESS estimate that is close to the number of distinct modes that are found. Bulk-ESS refers to the effective sample size based on the rank normalized draws. This does not directly compute the ESS relevant for computing the mean of the parameter, but instead computes a quantity that is well defined even if the chains do not have finite mean or variance. Overall bulk-ESS estimates the sampling efficiency for the location of the distribution (e.g. mean and median). Often quite smaller ESS would be sufficient for the desired estimation accuracy, but the estimation of ESS and convergence diagnostics themselves require higher ESS. We recommend requiring that the bulk-ESS is greater than 100 times the number of chains. For example, when running four chains, this corresponds to having a rank-normalized effective sample size of at least 400. Tail ESS Tail-ESS computes the minimum of the effective sample sizes (ESS) of the 5% and 95% quantiles. Tail-ESS can help diagnosing problems due to different scales of the chains and slow mixing in the tails. See also general information about ESS above in description of bulk-ESS. 11.2.3 Опять модель только со свободным членом Давайте посмотрим на наши данные: Если вам хочется вспомнить мем. Вот еще один. 11.2.4 Модель только с угловым коэффициентом get_prior(n_tokens ~ n_words+0, data = ud) fit_slope &lt;- brm(n_tokens ~ n_words+0, data = ud, cores = n_cores, refresh = 0, silent = TRUE) Running /usr/lib/R/bin/R CMD SHLIB foo.c gcc -I&quot;/usr/share/R/include&quot; -DNDEBUG -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/Rcpp/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/unsupported&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/BH/include&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/src/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppParallel/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -fpic -g -O2 -fdebug-prefix-map=/build/r-base-a3XuZ5/r-base-4.2.2.20221110=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -c foo.c -o foo.o In file included from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/Core:88, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/Dense:1, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13, from &lt;command-line&gt;: /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name ‘namespace’ 628 | namespace Eigen { | ^~~~~~~~~ /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:17: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘{’ token 628 | namespace Eigen { | ^ In file included from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/Dense:1, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13, from &lt;command-line&gt;: /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.2/RcppEigen/include/Eigen/Core:96:10: fatal error: complex: No such file or directory 96 | #include &lt;complex&gt; | ^~~~~~~~~ compilation terminated. make: *** [/usr/lib/R/etc/Makeconf:169: foo.o] Error 1 fit_slope Family: gaussian Links: mu = identity; sigma = identity Formula: n_tokens ~ n_words + 0 Data: ud (Number of observations: 20282) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS n_words 0.53 0.00 0.53 0.53 1.00 4864 3178 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 42.79 0.21 42.38 43.21 1.00 1182 1410 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). plot(fit_slope) Давайте посмотрим на предсказания модели в том, виде, в каком их может интерпретировать не специалист по байесовской статистике: library(tidybayes) ud %&gt;% add_epred_draws(fit_slope, ndraws = 50) %&gt;% ggplot(aes(n_words, n_tokens))+ geom_point(alpha = 0.01)+ stat_lineribbon(aes(y = .epred), color = &quot;red&quot;) 11.2.5 Модель с угловым коэффициентом и свободным членом get_prior(n_tokens ~ n_words, data = ud) fit_slope_intercept &lt;- brm(n_tokens ~ n_words, data = ud, cores = n_cores, refresh = 0, silent = TRUE) Running /usr/lib/R/bin/R CMD SHLIB foo.c gcc -std=gnu99 -std=gnu11 -I&quot;/usr/share/R/include&quot; -DNDEBUG -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/Rcpp/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/unsupported&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/BH/include&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/StanHeaders/include/src/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/StanHeaders/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppParallel/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -fpic -g -O2 -fdebug-prefix-map=/build/r-base-i2PIHO/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g -c foo.c -o foo.o In file included from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/Eigen/Core:88, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/Eigen/Dense:1, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13, from &lt;command-line&gt;: /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name ‘namespace’ 628 | namespace Eigen { | ^~~~~~~~~ /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:17: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘{’ token 628 | namespace Eigen { | ^ In file included from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/Eigen/Dense:1, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13, from &lt;command-line&gt;: /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/Eigen/Core:96:10: fatal error: complex: No such file or directory 96 | #include &lt;complex&gt; | ^~~~~~~~~ compilation terminated. make: *** [/usr/lib/R/etc/Makeconf:168: foo.o] Error 1 fit_slope_intercept Family: gaussian Links: mu = identity; sigma = identity Formula: n_tokens ~ n_words Data: ud (Number of observations: 20282) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 24.31 0.37 23.60 25.03 1.00 2919 2740 n_words 0.48 0.00 0.48 0.48 1.00 4859 2569 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 39.05 0.19 38.67 39.44 1.00 2290 2065 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). plot(fit_slope_intercept) ud %&gt;% add_epred_draws(fit_slope_intercept, ndraws = 50) %&gt;% ggplot(aes(n_words, n_tokens))+ geom_point(alpha = 0.01)+ stat_lineribbon(aes(y = .epred), color = &quot;red&quot;) 11.2.6 Модель со смешанными эффектами В данных есть группировка по языкам, которую мы все это время игнорировали. Давайте сделаем модель со смешанными эффектами: get_prior(n_tokens ~ n_words+(1|language), data = ud) fit_mixed &lt;- brm(n_tokens ~ n_words + (1|language), data = ud, cores = n_cores, refresh = 0, silent = TRUE) Running /usr/lib/R/bin/R CMD SHLIB foo.c gcc -std=gnu99 -std=gnu11 -I&quot;/usr/share/R/include&quot; -DNDEBUG -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/Rcpp/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/unsupported&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/BH/include&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/StanHeaders/include/src/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/StanHeaders/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppParallel/include/&quot; -I&quot;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -fpic -g -O2 -fdebug-prefix-map=/build/r-base-i2PIHO/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g -c foo.c -o foo.o In file included from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/Eigen/Core:88, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/Eigen/Dense:1, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13, from &lt;command-line&gt;: /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name ‘namespace’ 628 | namespace Eigen { | ^~~~~~~~~ /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:17: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘{’ token 628 | namespace Eigen { | ^ In file included from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/Eigen/Dense:1, from /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13, from &lt;command-line&gt;: /home/agricolamz/R/x86_64-pc-linux-gnu-library/4.1/RcppEigen/include/Eigen/Core:96:10: fatal error: complex: No such file or directory 96 | #include &lt;complex&gt; | ^~~~~~~~~ compilation terminated. make: *** [/usr/lib/R/etc/Makeconf:168: foo.o] Error 1 fit_mixed Family: gaussian Links: mu = identity; sigma = identity Formula: n_tokens ~ n_words + (1 | language) Data: ud (Number of observations: 20282) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Group-Level Effects: ~language (Number of levels: 9) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Intercept) 64.03 18.87 38.13 111.85 1.01 558 1054 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 5.91 20.69 -36.51 46.11 1.00 642 824 n_words 0.49 0.00 0.49 0.49 1.00 4075 2333 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 29.28 0.14 29.01 29.55 1.00 2316 1988 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). plot(fit_mixed) ud %&gt;% add_epred_draws(fit_mixed, ndraws = 50) %&gt;% ggplot(aes(n_words, n_tokens))+ geom_point(alpha = 0.01)+ stat_lineribbon(aes(y = .epred), color = &quot;red&quot;) + facet_wrap(~language) То, что получилось учитывает общий эффект всех языков: посмотрите на каталанский. Если построить модель по каждому языку, то получится совсем другая картина: ud %&gt;% ggplot(aes(n_words, n_tokens))+ geom_smooth(method = &quot;lm&quot;) + geom_point(alpha = 0.3)+ facet_wrap(~language) В работе (Coretta 2016) собраны данные длительности исландских гласных. Используя байесовскую регрессию с априорными распределениями по умолчанию, смоделируйте длительность гласного (vowel.dur) в зависимости от аспирированности (aspiration) учитывая эффект носителя. Визуализируйте результаты модели. Ссылки на литературу "],["ссылки-на-литературу.html", "Ссылки на литературу", " Ссылки на литературу "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
