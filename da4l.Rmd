--- 
title: "Анализ данных для лингвистов"
author: "Г. А. Мороз"
bibliography:
- bibliography.bib
editor_options:
  chunk_output_type: console
link-citations: yes
documentclass: book
site: bookdown::bookdown_site
biblio-style: apalike
---

# О курсе

Материалы для курса Анализа данных для лингвистов, Школа лингвистики НИУ ВШЭ.

* вместо первой лекции можно посмотреть [запись лекции 2021.01.13](https://youtu.be/HmLcBJnfipk)
* вместо второй лекции можно посмотреть [запись лекции 2021.01.15](https://youtu.be/V_c_K_wBMuY)
* вместо третей лекции можно посмотреть [запись лекции 2021.01.20](https://youtu.be/cRc5z9F3XNw)
* вместо четвертой лекции можно посмотреть [запись лекции 2021.01.22](https://youtu.be/zj-1-invsGM)
* вместо пятой лекции можно посмотреть [запись лекции 2021.01.27](https://www.youtube.com/watch?v=2YRAdMLT4N0&list=PL_HB2SrKjmMJ-dUBtXUSQBZxRftJJX5uL&index=5)
* вместо шестой лекции можно посмотреть [запись лекции 2021.01.29](https://www.youtube.com/watch?v=FA-_Qbpmb6s&list=PL_HB2SrKjmMJ-dUBtXUSQBZxRftJJX5uL&index=6)
* вместо седьмой лекции можно посмотреть [запись лекции 2021.02.03](https://youtu.be/DkNnGeJT2K0)
* вместо восьмой лекции можно посмотреть [запись лекции 2021.02.05](https://youtu.be/c2mej30JnRM)
* вместо девятой лекции можно посмотреть [запись лекции 2021.02.10](https://youtu.be/9ltNRMQmt0U)
* вместо десятой лекции можно посмотреть [запись лекции 2021.02.12](https://youtu.be/zCG_edAziVk)
* вместо одиннадцатой лекции можно посмотреть [запись лекции 2021.02.19](https://youtu.be/6bERf7DMvfg)
* вместо двеннадцатой лекции можно посмотреть [запись лекции 2021.02.24](https://youtu.be/I80UaVOmpxY)
* вместо триннадцатой и четырнадцатой лекций можно посмотреть вот эти видео: [часть 1](https://youtu.be/_IltMx042DE), [часть 2](https://youtu.be/1qbk5dATogc)
* вместо пятнадцатой лекции можно посмотреть [запись лекции 2021.03.03](https://www.youtube.com/watch?v=mIhT4y4FigY&list=PL_HB2SrKjmMJ-dUBtXUSQBZxRftJJX5uL&index=12)
* вместо шестнадцатой лекции можно посмотреть
[запись лекции 2021.03.05](https://www.youtube.com/watch?v=mIhT4y4FigY&list=PL_HB2SrKjmMJ-dUBtXUSQBZxRftJJX5uL&index=12) и
[другую](https://www.youtube.com/watch?v=m06AOJBU9Bo&list=PL_HB2SrKjmMJ-dUBtXUSQBZxRftJJX5uL&index=11) и
 [третью](https://www.youtube.com/watch?v=XkEm4SHcYbg&list=PL_HB2SrKjmMJ-dUBtXUSQBZxRftJJX5uL&index=13), 
* вместо шестнадцатой лекции можно посмотреть вот это [видео](https://youtu.be/a75VjnY-w0s).

## Используемые пакеты

```{r}
packageVersion("tidyverse")
packageVersion("fitdistrplus")
packageVersion("mixtools")
packageVersion("lme4")
packageVersion("lmerTest")
packageVersion("car")
packageVersion("pscl")
packageVersion("nnet")
packageVersion("MASS")
packageVersion("ggeffects")
packageVersion("brms")
packageVersion("tidybayes")
```

## Домашние задания

* домашнее задание к лекции 16.01.2023:
    * вспомните пожалуйста, условные вероятности, формулу Байеса и при каких условиях ее применяют;
    * посмотрите освежающие материалы про [условную вероятность](http://setosa.io/conditional/) и [формулу Байеса](https://www.youtube.com/watch?v=HZGCoVF3YvM).

<!--chapter:end:index.Rmd-->

---
editor_options: 
  chunk_output_type: console
---


```{r setup01, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, comment = "")
options(scipen=999)
library(tidyverse)
theme_set(theme_bw())
```

# Распределения

```{r}
library(tidyverse)
```


## Распределения в R

В R встроено какое-то количество известных распределений. Все они представлены четырьмя функциями: 

* `d...` (функция плотности, probability density function),
* `p...` (функция распределения, cumulative distribution function) --- интеграл площади под кривой от начала до указанной квантили
* `q...` (обратная функции распределения, inverse cumulative distribution function) --- значение *p*-той квантили распределения
* и `r...` (рандомные числа из заданного распределения).

Рассмотрим все это на примере нормального распределения.

```{r}
tibble(x = 1:100,
       PDF = dnorm(x = x, mean = 50, sd = 10)) %>% 
  ggplot(aes(x, PDF))+
  geom_point()+
  geom_line()+
  labs(title = "PDF нормального распределения (μ = 50, sd = 10)")

tibble(x = 1:100,
       CDF = pnorm(x, mean = 50, sd = 10)) %>% 
  ggplot(aes(x, CDF))+
  geom_point()+
  geom_line()+
  labs(title = "CDF нормального распределения (μ = 50, sd = 10)")

tibble(quantiles = seq(0, 1, by = 0.01),
       value = qnorm(quantiles, mean = 50, sd = 10)) %>% 
  ggplot(aes(quantiles, value))+
  geom_point()+
  geom_line()+
  labs(title = "inverse CDF нормального распределения (μ = 50, sd = 10)")

tibble(sample = rnorm(100, mean = 50, sd = 10)) %>% 
  ggplot(aes(sample))+
  geom_histogram()+
  labs(title = "выборка нормально распределенных чисел (μ = 50, sd = 10)")
```

Если не использовать `set.seed()`, то результат работы рандомизатора нельзя будет повторить.

```{block, type = "rmdtask"}
Какое значение имеет 25% квантиль нормального распределения со средним в 20 и стандартным отклонением 90? Ответ округлите до трех знаков после запятой.
```

```{r, echo=FALSE, results='asis'}
library(checkdown)
qnorm(0.25, mean = 20, sd = 90) %>% 
  round(3) %>% 
  check_question()
```

```{block, type = "rmdtask"}
Данные из базы данных фонетических инвентарей PHOIBLE [@phoible], достаточно сильно упрощая, можно описать нормальным распределением со средним 35 фонем и стандартным отклонением 13. Если мы ничего не знаем про язык, оцените с какой вероятностью, согласно этой модели произвольно взятый язык окажется в промежутке между 25 и 50 фонемами? Ответ округлите до трех знаков после запятой.
```
```{r, echo=FALSE, results='asis'}
round(pnorm(50, 35, 13) - pnorm(25, 35, 13), 3) %>%
  check_question()
```

```{block, type = "rmdtask"}
Какие есть недостатки у модели из предыдущего задания?
```

```{r, echo=FALSE, results='asis'}
check_hints(list_title = "ответы:",
            hint_text = c("Согласно данной модели, количество фонем в языках может принимать значение равное любому действительному числу, в то время как в данных мы наблюдаем только целые числа.",
                          paste0("Данная модель предсказывает, что количество фонем ",
                                 round(pnorm(0, 35, 13)*100, 1),
                                 "% языков  меньше нуля, что представляется невозможным с точки зрения лингвистики.")),
            hint_title = paste0("недостаток ", 1:2))
```


## Дискретные переменные
### Биномиальное распределение

Биномиальное распределение --- распределение количетсва успехов эксперементов Бернулли из *n* попыток с вероятностью успеха *p*.

$$P(k | n, p) = \frac{n!}{k!(n-k)!} \times p^k \times (1-p)^{n-k} =  {n \choose k} \times p^k \times (1-p)^{n-k}$$ 
$$ 0 \leq p \leq 1; n, k > 0$$

```{r}
tibble(x = 0:50,
       density = dbinom(x = x, size = 50, prob = 0.16)) %>% 
  ggplot(aes(x, density))+
  geom_point()+
  geom_line()+
  labs(title = "Биномиальное распределение p = 0.16, n = 50")
```

```{block, type = "rmdtask"}
Немного упрощая данные из статьи [@rosenbach03: 394], можно сказать что носители британского английского предпочитают *s*-генитив (90%) *of*-генитиву (10%). Какова вероятность, согласно этим данным, что в интервью британского актера из 118 контекстов будет 102 *s*-генитивов? Ответ округлите до трёх ИЛИ МЕНЕЕ знаков после запятой.
```

```{r, echo=FALSE, results='asis'}
round(dbinom(x = 102, size = 118, prob = 0.9), 3) %>% 
  check_question()
```

```{block, type = "rmdtask"}
А какое значение количества *s*-генитивов наиболее ожидаемо, согласно этой модели?
```

```{r, echo=FALSE, results='asis'}
which.max(round(dbinom(x = 1:118, size = 118, prob = 0.9), 3)) %>% 
  check_question()
```


### Геометрическое распределение

Геометрическое распределение --- распределение количетсва эксперементов Бернулли с вероятностью успеха *p* до первого успеха.

$$P(k | p) = (1-p)^k\times p$$
$$k\in\{1, 2, \dots\}$$

```{r}
tibble(x = 0:50,
       density = dgeom(x = x, prob = 0.16)) %>% 
  ggplot(aes(x, density))+
  geom_point()+
  geom_line()+
  labs(title = "Геометрическое распределение p = 0.16, n = 50")
```

```{block, type = "rmdtask"}
Приняв модель из [@rosenbach03: 394], какова вероятность, что в интервью с британским актером первый *of*-генитив будет третьим по счету? 
```

```{r, echo=FALSE, results='asis'}
round(dgeom(x = 3-1, prob = 0.1), 3) %>% 
  check_question()
```

### Распределение Пуассона

Распределение дискретной переменной, обозначающей количество случаев $k$ некоторого события, которое происходит с некоторой заданной частотой $\lambda$.

$$P(\lambda) = \frac{e^{-\lambda}\times\lambda^k}{k!}$$

```{r}
tibble(k = 0:50,
       density = dpois(x = k, lambda = 5)) %>% 
  ggplot(aes(k, density))+
  geom_point()+
  geom_line()+
  labs(title = "Распределение Пуассона с параметром λ = 5")
```

Параметр $\lambda$ в модели Пуассона одновременно является и средним, и дисперсией.

Попробуем воспользоваться распределением Пуассона для моделирования количества слогов в андийском языке. Количество слогов -- это всегда натуральное число (т. е. не бывает 2.5 слогов, не бывает -3 слогов и т. д., но в теории может быть 0 слогов), так что модель Пуассона здесь применима. Согласно модели Пуассона все слова независимо друг от друга получают сколько-то слогов согласно распределению Пуассона. Посмотрим на данные:

```{r}
andic_syllables <- read_csv("https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/andic_syllables.csv") 

andic_syllables %>% 
  ggplot(aes(n_syllables, count))+
  geom_col()+
  facet_wrap(~language, scales = "free")
```


```{r, include=FALSE}
andic_syllables %>% 
  filter(language == "Andi") %>% 
  uncount(count) %>% 
  summarise(est = fitdistrplus::fitdist(n_syllables, distr = 'pois', method = 'mle')$estimate) %>% 
  pull() %>% 
  round(3) ->
  est_lambda
```

Птичка напела (мы научимся узнавать, откуда птичка это знает на следующем занятии), что андийские данные можно описать при помощи распределения Пуассона с параметром $\lambda$ = `r est_lambda`.

```{r}
andic_syllables %>% 
  filter(language == "Andi") %>% 
  rename(observed = count) %>% 
  mutate(predicted = dpois(n_syllables, lambda = 2.783)*sum(observed)) %>% 
  pivot_longer(names_to = "type", values_to = "value", cols = c(observed, predicted)) %>% 
  ggplot(aes(n_syllables, value, fill = type))+
  geom_col(position = "dodge")
```

```{block, type = "rmdtask"}
На графиках ниже представлены предсказания трех Пуассоновских моделей, какая кажется лучше?
```

```{r, echo = FALSE, fig.height=8}
andic_syllables %>% 
  filter(language == "Bagvalal") %>% 
  rename(observed = count) %>% 
  mutate(predicted = dpois(n_syllables, lambda = 1.452)*sum(observed)) %>% 
  pivot_longer(names_to = "type", values_to = "value", cols = c(observed, predicted)) %>% 
  ggplot(aes(n_syllables, value, fill = type))+
  geom_col(position = "dodge")+
  labs(title = "Багвалинские данные, λ = 1.452",
       y = "",
       x = "количество слогов") ->
  p1

andic_syllables %>% 
  filter(language == "Bagvalal") %>% 
  rename(observed = count) %>% 
  mutate(predicted = dpois(n_syllables, lambda = 2.452)*sum(observed)) %>% 
  pivot_longer(names_to = "type", values_to = "value", cols = c(observed, predicted)) %>% 
  ggplot(aes(n_syllables, value, fill = type))+
  geom_col(position = "dodge")+
  labs(title = "Багвалинские данные, λ = 2.452",
       y = "",
       x = "количество слогов")->
  p2

andic_syllables %>% 
  filter(language == "Bagvalal") %>% 
  rename(observed = count) %>% 
  mutate(predicted = dpois(n_syllables, lambda = 3.452)*sum(observed)) %>% 
  pivot_longer(names_to = "type", values_to = "value", cols = c(observed, predicted)) %>% 
  ggplot(aes(n_syllables, value, fill = type))+
  geom_col(position = "dodge")+
  labs(title = "Багвалинские данные, λ = 3.452",
       y = "",
       x = "количество слогов")->
  p3

plot(gridExtra::arrangeGrob(p1, p2, p3))
```

```{block, type = "rmdtask"}
Выше было написано:

> Согласно модели Пуассона все слова **независимо друг от друга** получают сколько-то слогов согласно распределению Пуассона.

Какие проблемы есть у предположения о независимости друг от друга количества слогов разных слов в словаре?
```

## Числовые переменные
### Нормальное распределение

$$P(x) = \frac{1}{\sigma\sqrt{2\pi}}\times e^{-\frac{\left(x-\mu\right)^2}{2\sigma^2}}$$

$$\mu \in \mathbb{R}; \sigma^2 > 0$$

```{r}
tibble(x = 1:100,
       PDF = dnorm(x = x, mean = 50, sd = 10)) %>% 
  ggplot(aes(x, PDF))+
  geom_point()+
  geom_line()+
  labs(title = "PDF нормального распределения (μ = 50, sd = 10)")
```

```{r, include=FALSE}
vowels <- read_csv("https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/phonTools_hillenbrand_1995.csv") 

vowels %>% 
  summarise(est = fitdistrplus::fitdist(dur, distr = 'norm', method = 'mle')$estimate) %>% 
  pull() %>% 
  round(3)->
  est_m_sd
```

Птичка напела, что длительность гласных американского английского из [@hillenbrand95] можно описать нормальным распределением с параметрами $\mu =$ `r est_m_sd[1]` и $\sigma =$ `r est_m_sd[2]`. Посмотрим, как можно совместить данные и это распределение:

```{r}
vowels <- read_csv("https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/phonTools_hillenbrand_1995.csv") 
vowels %>% 
  ggplot(aes(dur)) + 
  geom_histogram(aes(y =..density..)) + # обратите внимание на аргумент ..density..
  stat_function(fun = dnorm, args = list(mean = 274.673, sd = 64.482), color = "red")
```

### Логнормальное распределение

$$P(x) = \frac{1}{\sqrt{x\sigma2\pi}}\times e^{-\frac{\left(\ln(x)-\mu\right)^2}{2\sigma^2}}$$

$$\mu \in \mathbb{R}; \sigma^2 > 0$$

```{r}
tibble(x = 1:100,
       PDF = dlnorm(x = x, mean = 3, sd = 0.5)) %>% 
  ggplot(aes(x, PDF))+
  geom_point()+
  geom_line()+
  labs(title = "PDF логнормального распределения (μ = 3, σ = 0.5)")
```

```{r, include=FALSE}
vowels %>% 
  summarise(est = fitdistrplus::fitdist(dur, distr = 'lnorm', method = 'mle')$estimate) %>% 
  pull() %>% 
  round(3)->
  est_ml_sdl
```

```{block, type = "rmdtask"}
Какая из логнормальных моделей для длительности гласных американского английского из [@hillenbrand95] лучше подходит к данным? Попробуйте самостоятельно построить данный график.
```

```{r, echo = FALSE}
vowels %>% 
  ggplot(aes(dur)) + 
  geom_histogram(aes(y =..density..), fill = "grey90") + 
  stat_function(fun = dlnorm, args = list(mean = 5.587, sd = 0.242), color = "red")+
  stat_function(fun = dlnorm, args = list(mean = 5.687, sd = 0.342), color = "darkgreen")+
  stat_function(fun = dlnorm, args = list(mean = 5.487, sd = 0.262), color = "navy")+
  labs(subtitle = "синяя: ln μ = 5.487, ln σ = 0.262\nкрасная: ln μ = 5.687, ln σ = 0.342\nзеленая: ln μ = 5.487, ln σ = 0.262\n",
       x = "длительность гласного (мс)",
       y = "значение функции плотности")
```

### Что еще почитать про распределения?

Люди придумали очень много разных распределений. Стоит, наверное, также понимать, что распределения не существуют отдельно в вакууме: многие из них математически связаны друг с другом. Про это можно посмотреть [вот здесь](http://www.math.wm.edu/~leemis/chart/UDR/UDR.html) или [здесь](https://en.wikipedia.org/wiki/Relationships_among_probability_distributions).

<!--chapter:end:01-distributions.Rmd-->

---
editor_options: 
  chunk_output_type: console
---

```{r setup02, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, comment = "")
options(scipen=999)
library(tidyverse)
theme_set(theme_bw())
```

# Метод максимального правдоподобия

## Оценка вероятности

```{r}
library(tidyverse)
```

```{r, include=FALSE}
vowels <- read_csv("https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/phonTools_hillenbrand_1995.csv") 

vowels %>% 
  summarise(est = fitdistrplus::fitdist(dur, distr = 'lnorm', method = 'mle')$estimate) %>% 
  pull() %>% 
  round(3)->
  est_ml_sdl
```

Когда у нас задано некоторое распределение, мы можем задавать к нему разные вопросы. Например, если мы верим что длительность гласных американского английского из [@hillenbrand95] можно описать логнормальным распределением с параметрами $\ln{\mu} =$ `r est_ml_sdl[1]` и $\ln{\sigma} =$ `r est_ml_sdl[2]`, то мы можем делать некотрые предсказания относительно интересующей нас переменной. 

```{r}
ggplot() + 
  stat_function(fun = dlnorm, args = list(mean = 5.587, sd = 0.242))+
  scale_x_continuous(breaks = 0:6*100, limits = c(0, 650))+
  labs(x = "длительность гласного (мс)",
       y = "значение функции плотности")
```


```{block, type = "rmdtask"}
Если принять на веру, что логнормальное распределение с параметрами $\ln{\mu} =$ 5.587 и $\ln{\sigma}=$ 0.242 описывает данные длительности гласных американского английского из [@hillenbrand95], то какова вероятность наблюдать значения между 300 и 400 мс? То же самое можно записать, используя математическую нотацию:

$$P\left(X \in [300,\, 400] | X \sim \ln{\mathcal{N}}(\ln{\mu} = 5.587, \ln{\sigma}=0.242)\right) = ??$$
Ответ округлите до трех и меньше знаков после запятой.

```

```{r, echo = FALSE}
ggplot() + 
  stat_function(fun = dlnorm, args = list(mean = 5.587, sd = 0.242))+
  stat_function(fun = dlnorm, args = list(mean = 5.587, sd = 0.242), 
                xlim = c(300, 400), geom = "area", fill = "lightblue")+
  scale_x_continuous(breaks = 0:6*100, limits = c(0, 650))+
  labs(x = "длительность гласного (мс)", y = "значение функции плотности")
```

```{r, echo = FALSE, results = 'asis'}
library(checkdown)
round(plnorm(400, mean = 5.587, sd = 0.242) - plnorm(300, mean = 5.587, sd = 0.242), 3) %>% 
  check_question()
```


```{block, type = "rmdtask"}
Если принять на веру, что биномиальное распределение с параметрами $p =$ 0.9 описывает, согласно [@rosenbach03: 394] употребление *s*-генитивов в британском английском, то какова вероятность наблюдать значения между 300 и 350 генитивов в интервью, содержащее 400 генитивных контекстов? То же самое можно записать, используя математическую нотацию:

$$P\left(X \in [300,\, 350] | X \sim Binom(n = 400, p = 0.9)\right) = ??$$
Ответ округлите до трех и меньше знаков после запятой.
```

```{r, echo = FALSE, results = 'asis'}
# round(pbinom(350, size = 400, prob = 0.9) - pbinom(300, size = 400, prob = 0.9), 3)
round(sum(dbinom(300:350, size = 400, prob = 0.9)), 3) %>% 
  check_question()
```

## Функция правдоподобия

Если при поиске вероятностей, мы предполагали, что данные нам **неизвестны**, а распределение и его параметры **известны**, то функция правдоподобия позволяет этот процесс перевернуть, запустив поиск параметров распределения, при изветсных данных и семье распределения:

$$L\left(X \sim Distr(...)|x\right) = ...$$

Таким образом получается, что на основании функции плотности мы можем сравнивать, какой параметр лучше подходит к нашим данным.

Для примера рассмотрим наш s-генетив: мы провели интервью и нам встретилось 85 *s*-генетивов из 100 случаев всех генетивов. Насколько хорошо подходит нам распределение с параметром *p* = 0.9?

```{r, echo = FALSE}
tibble(x = 0:100) %>% 
  ggplot(aes(x)) +
  stat_function(fun = function(x) dbinom(x, 100, 0.9), geom = "col")+
  geom_segment(aes(x = 85, xend = 85, y = 0, yend = dbinom(85, 100, 0.9)), color = "red")+
  geom_segment(aes(x = 85, xend = 0, y = dbinom(85, 100, 0.9), yend = dbinom(85, 100, 0.9)), color = "red", arrow = arrow(length = unit(0.02, "npc")))+
  scale_x_continuous(breaks = c(0:5*20, 85))+
  scale_y_continuous(breaks = c(0:3*0.05, round(dbinom(85, 100, 0.9), 3)))+
  labs(x = "Количество s-генитивов в интервью")
```

Ответ:

```{r}
dbinom(85, 100, 0.9)
```

Представим теперь это как функцию от параметра *p*:

```{r}
tibble(p = seq(0, 1, by = 0.01)) %>% 
  ggplot(aes(p)) +
  stat_function(fun = function(p) dbinom(85, 100, p), geom = "col")+
  labs(x = "параметр биномиального распределения p",
       y = "значение функции правдоподобия\n(одно наблюдение)")
```

А что если мы располагаем двумя интервью одного актера? В первом на сто генитивов пришлось 85 s-генитивов, а во втором -- 89. В таком случае, также как и с вероятностью наступления двух независимых событий, значения функции плотности перемножаются.

```{r}
dbinom(85, 100, 0.9)*dbinom(89, 100, 0.9)
```


```{r}
tibble(p = seq(0, 1, by = 0.01)) %>% 
  ggplot(aes(p)) +
  stat_function(fun = function(p) dbinom(85, 100, p)*dbinom(89, 100, p), geom = "col")+
  labs(x = "параметр биномиального распределения p",
       y = "значение функции правдоподобия\n(два наблюдения)")
```

В итоге:

* вероятность --- P(data|distribution)
* правдоподобие --- L(distribution|data)

Интеграл распределения/сумма значений вероятностей равен/на 1. [Интеграл распределения/сумма значений правдоподобия может быть не равен/на 1](https://stats.stackexchange.com/a/31241/225843).

## Пример с непрерывным распределением

Мы уже обсуждали, что длительность гласных американского английского из [@hillenbrand95] можно описать логнормальным распределением с параметрами $\ln\mu$  и $\ln\sigma$. Предположим, что $\ln\sigma = 0.342$, построим функцию правдоподобия для $\ln\mu$:

```{r}
vowels <- read_csv("https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/phonTools_hillenbrand_1995.csv") 

tibble(ln_mu = seq(5, 6, by = 0.001)) %>% 
  ggplot(aes(ln_mu)) + 
  stat_function(fun = function(ln_mu) dlnorm(vowels$dur[1], meanlog = ln_mu, sdlog = 0.242))+
  labs(x = "параметр логнормального распределения ln μ",
       y = "значение функции правдоподобия\n(одно наблюдение)")

tibble(ln_mu = seq(5, 6, by = 0.001)) %>% 
  ggplot(aes(ln_mu)) + 
  stat_function(fun = function(ln_mu) dlnorm(vowels$dur[1], meanlog = ln_mu, sdlog = 0.242)*dlnorm(vowels$dur[2], meanlog = ln_mu, sdlog = 0.242))+
  labs(x = "параметр логнормального распределения ln μ",
       y = "значение функции правдоподобия\n(два наблюдения)")
tibble(ln_mu = seq(5, 6, by = 0.001)) %>% 
  ggplot(aes(ln_mu)) + 
  stat_function(fun = function(ln_mu) dlnorm(vowels$dur[1], meanlog = ln_mu, sdlog = 0.242)*dlnorm(vowels$dur[2], meanlog = ln_mu, sdlog = 0.242)*dlnorm(vowels$dur[3], meanlog = ln_mu, sdlog = 0.242))+
  labs(x = "параметр логнормального распределения ln μ",
       y = "значение функции правдоподобия\n(три наблюдения)")
```

Для простоты в начале я зафиксировал один из параметров логнормального распредления: лог стандартное отклонение. Конечно, это совсем необязательно делать: можно создать матрицу значений лог среднего и лог стандартного отклонения и получить для каждой ячейки матрицы значения функции правдоподобия.

## Метод максимального правдоподобия (MLE)

Функция правдоподобия позволяет подбирать параметры распределения. Оценка параметров распределения при помощи функции максимального правдоподобия получила название метод максимального правдоподобия. Его я и использовал ранее для того, чтобы получить значения распределений для заданий из первого занятия:

* данные длительности американских гласных из [@hillenbrand95] и логнормальное распределение
```{r}
fitdistrplus::fitdist(vowels$dur, distr = 'lnorm', method = 'mle')
```

* количество андийских слогов в словах и распределение Пуассона
```{r}
andic_syllables <- read_csv("https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/andic_syllables.csv") 

andic_syllables %>% 
  filter(language == "Andi") %>% 
  uncount(count) %>% 
  pull(n_syllables) %>% 
  fitdistrplus::fitdist(distr = 'pois', method = 'mle')
```

* Есть и другие методы оценки параметров.
* Метод максимального правдоподобия может быть чувствителен к размеру выборки.

```{block, type = "rmdtask"}
Отфильтруйте из [данных с количеством слогов в андийских языках](https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/andic_syllables.csv) багвалинский и, используя метод максимального правдоподобия, оцените для них параметры модели Пуассона.
```

```{block, type = "rmdtask"}
В работе [@coretta2016] собраны [данные](https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/Coretta_2017_icelandic.csv) длительности исландских гласных. Отфильтруйте данные, оставив односложные слова (переменная `syllables`) после придыхательного (переменная `aspiration`), произнесенные носителем `tt01` (переменная `speaker`) и постройте следующий график, моделируя длительность гласных (переменная `vowel.dur`) нормальным и логнормальным распределением. Как вам кажется, какое распределение лучше подходит к данным? Докажите ваше утверждение, сравнив значения правдоподобия. 
```

```{r, echo = FALSE}
df <- read_csv("https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/Coretta_2017_icelandic.csv")

df %>% 
  filter(speaker == "tt01",
         syllables == "mono",
         aspiration == "yes") %>% 
  pull(vowel.dur) ->
  vowel_dur

lnorm_params <- fitdistrplus::fitdist(vowel_dur, distr = 'lnorm', method = 'mle')
norm_params <- fitdistrplus::fitdist(vowel_dur, distr = 'norm', method = 'mle')

tibble(vowel_dur) %>% 
  ggplot(aes(vowel_dur))+
  geom_density()+
  stat_function(fun = dlnorm, args = list(lnorm_params$estimate[1], lnorm_params$estimate[2]), color = "red")+
  stat_function(fun = dnorm, args = list(norm_params$estimate[1], norm_params$estimate[2]), color = "darkgreen")+
  labs(subtitle = "Черная линия -- данные,\nзеленая и красная линии -- нормальное и логнормальное распределения соответственно,\nполученные методом максимального правдоподобия",
       x = "длительность гласных", 
       y = "функция плотности",
       caption = "данные [Coretta 2016]")

# prod(dnorm(vowel_dur, norm_params$estimate[1], norm_params$estimate[2]))/prod(dlnorm(vowel_dur, lnorm_params$estimate[1], lnorm_params$estimate[2]))
```


## Логорифм функции правдоподобия

Так как в большинстве случаев нужно найти лишь максимум функции правдоподобия, а не саму функцию $\ell(x|\theta)$, то для облегчения подсчетов используют логорифмическую функцию правдоподобия $\ln\ell(x|\theta)$: в результате, вместо произведения появляется сумма[^log]:

$$\text{argmax}_\theta \prod \ell(\theta|x) = \text{argmax}_\theta \sum \ln\ell(\theta|x) $$

[^log]: Это просто свойство логарифмов: `log(5*5) = log(5)+log(5)`

Во всех предыдущих примерах мы смотрели на 1-3 примера данных, давайте попробуем использовать функцию правдоподобия для большего набора данных.

```{block, type = "rmdtask"}
Представим, что мы проводим некоторый эксперимент, и у некоторых участников все получается с первой попытки, а некоторым нужна еще одна попытка или даже две. Дополните код функциями правдоподобия и логорифмической функцией правдоподобия, чтобы получился график ниже.
```

```{r, eval = FALSE}
set.seed(42)
v <- sample(0:2, 10, replace = TRUE)

sapply(seq(0.01, 0.99, 0.01), function(p){
  ...
}) ->
  likelihood

sapply(seq(0.01, 0.99, 0.01), function(p){
  ...
}) ->
  loglikelihood

tibble(p = seq(0.01, 0.99, 0.01),
       loglikelihood,
       likelihood) %>% 
  pivot_longer(names_to = "type", values_to = "value", loglikelihood:likelihood) %>% 
  ggplot(aes(p, value))+
  geom_line()+
  geom_vline(xintercept = 0.33, linetype = 2)+
  facet_wrap(~type, scales = "free_y", nrow = 2)+
  scale_x_continuous(breaks = c(0:5*0.25, 0.33))
```



```{r, echo = FALSE}
set.seed(42)
v <- sample(0:2, 10, replace = TRUE)

sapply(seq(0.01, 0.99, 0.01), function(p){
  prod(dgeom(v, prob = p))
}) ->
  likelihood

sapply(seq(0.01, 0.99, 0.01), function(p){
  sum(log(dgeom(v, prob = p)))
}) ->
  loglikelihood

tibble(p = seq(0.01, 0.99, 0.01),
       loglikelihood,
       likelihood) %>% 
  pivot_longer(names_to = "type", values_to = "value", loglikelihood:likelihood) %>% 
  ggplot(aes(p, value))+
  geom_line()+
  geom_vline(xintercept = 0.59, linetype = 2)+
  facet_wrap(~type, scales = "free_y", nrow = 2)+
  scale_x_continuous(breaks = c(0:5*0.25, 0.59))
```


<!--chapter:end:02-likelihood.Rmd-->

`r if (knitr::is_html_output()) '
# Ссылки на литературу {-}
'`

<!--chapter:end:20-references.Rmd-->

